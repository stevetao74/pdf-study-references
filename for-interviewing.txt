;{   高并发高可用 高并发系统设计40问
    单线程epoll就可以做到单台百万并发连接
        0. ulimit open-files = 1 000 000
        1. cat /proc/sys/fs/file-max
            sysctl -p 生效
        2. nf_conntrack_max nf_conntrack模块在跟踪连接的时候需要创建一个hash表存储连接, 太小了,导致新来的连接存不了
        3. 连接耗时: 扩大listen队列 accept并发
    
    1. 开篇词 | 为什么你要学习高并发系统设计?
        · 低并发下只需要了解基本的使用方式,高并发下要关注缓存命中率、缓存击穿、雪崩、缓存一致性
        · 消息队列解耦消息生产方和消费方,减少两方处理速度不一致问题
            · 这里不仅仅为了处理速度不一致的问题
            · 最基本的是 系统模块要遵循 高内聚、低耦合这个基本设计思想

    2. 高并发系统:它的通用设计方法是什么?
        · Scale-out 横向扩展
            · 分布式的机器
                · Scale-up 提升单台服务器的处理能力
                    · 优先实现Scale-up, 超过极限后实现Scale-out
                · Scale-out 增加服务器的数量
                    · 节点故障后的整体可用性
                    · 多个节点有状态要同步,如何保证信息在不同节点的一致性
                    · 在客户端无感知的情况下,删除增加服务器
        · 缓存
            · 使请求快速的得到响应
        · 异步
            请求到来,处理来不及,可以让请求先返回,避免客户端等待;数据准备好以后再发给请求方, 服务端也可以处理更多的请求
    4. 系统设计目标(一):如何提升系统性能?
        · 高并发系统设计的三大目标:高性能、高可用、可扩展
            · 高性能
                毫秒级响应
                · 性能优化
                    · 性能的度量指标
                        · 平均值
                            30s内/1000次请求的平均响应时间
                        · 最大值
                            30s内/1000次请求的最大响应时间
                        · 分位值
                            不同百分位的响应时间统计
                    · 提高系统的处理核心数
                    · 减少单次任务响应时间
                        · CPU密集
                            算法优化 //使用linux的perf或者eBPF来发现CPU时间消耗最多的模块
                        · IO密集
            · 高可用 High Availability, HA
                · 365天无故障
                · Hadoop 一次启动2个进程,一个处于active状态, 一个处于standby状态;两者共享存储,一个发生故障了,就切换
                · 可用性的度量 Availiability = MTBF/ (MTBF + MTTR)
                    · 五个九 99.999% * 365天 = 32s
                    · MTBF:Mean Time Between Failure
                        两次故障的间隔时间
                    · MTTR:Mean Time To Repair
                        故障的恢复时间
                · 高可用系统设计思路
                    1. 系统设计 design for failue
                        · failover 故障转移
                            · 同级转移 热备,可以提供服务
                            · 主备转移 冷备,刚切换到active状态
                                · 主备切换, 涉及分布式一致性算法
                                    Paxos、Raft
                            ·检测故障
                                心跳,客户端定时向服务端发送心跳包
                        · 超时控制
                            · 模块间的调用超时时间通过日志统计99%的响应时间来指定
                            · 尽量在最短的时间内得不到答复就返回
                        · 降级
                            · 关闭不紧要的功能
                        · 限流
                            · 限制请求的数量
                    2. 系统运维
                        · 灰度发布
                            10%部分的机器上发布新版本,观察性能数据在全量更新
                        · 故障演练
                            使某些部件出现故障,观察性能
            · 高可扩展
                1. 存储层的扩展性
                    · 按照业务内容将数据库进行拆分;分库分表;最好不要用事务,协调的成本会很高
                2. 业务层的扩展性
                    · 把相同业务的服务超分成单独的业务池,每个业务池出现瓶颈时,就扩展这个单独的业务池
                        · 继续划分为核心和非核心:非核心的可以降级处理,优先扩容核心池
    5. 缓存;数据库成为瓶颈后,动态数据的查询要如何加速?
        ·分布式缓存
        · 热点本地缓存
            遇到极端的热点数据查询的时候,热点本地缓存主要部署在应用服务器的代码中,用于阻挡热点查询对于分布式缓存节点或者数据库的压力;优势是不需要跨网络调度,速度极快,所以可以来阻挡短时间内的热点查询
        · 缓存的读写策略
            · Cache Aside(旁路缓存)策略
                · 写请求来了,先写数据库,再删掉缓存
                · 读请求来了,缓存没有击中,读数据库,写缓存;缓存击中,直接返回
                · 写数据库比读数据库要慢, 读数据库-写缓存要比写数据库先执行,可以看成是原子的
            · Read/Write Through (读穿/写穿)策略 
                · 这种比上面的Cache Aside少见
                · Write Through
                    · 写请求来了,缓存击中了,更新缓存,再由缓存组件更新到数据库;没有击中,就“write miss”
                        · miss之后, Write Allocate(按写分配).
                            先写入缓存相应的位置,再由缓存组件更新到数据库中
                        · miss之后, No Write Allocate(不按写分配).
                            不写缓存,直接更新到数据库中(一般选这个)
                · Read Through
                    先查询缓存中数据是否存在,如果存在则直接返回;如果不存在,则由缓存组件负责从数据库中同步加载数据到缓存再返回数据
            · Write Back(写回)策略
                · 这种策略被用在内存写数据到磁盘中
                · 这个策略的核心思想是在写入数据时只写入缓存,并且把缓存块儿标记为“脏”的。而脏块儿只有被再次使用时才会将其中的数据写入到后端存储中。 // 相比上面的同步,这里是异步的
                · 在“Write Miss”的情况下,我们采用的是“Write Allocate”的方式,也就是在写入后端存储的同时要写入缓存,这样我们在之后的写请求中都只需要更新缓存即可
                · 如果使用 Write Back 策略的话,读的策略也有一些变化了。我们在读取缓存时如果发现缓存命中则直接返回缓存数据。如果缓存不命中则寻找一个可用的缓存块儿,如果这个缓存块儿是“脏”的,就把缓存块儿中之前的数据写入到后端存储中,并且从后端存储加载数据到缓存块儿,如果不是脏的,则由缓存组件将后端存储中的数据加载到缓存中,最后我们将缓存设置为不是脏的,返回数据就好了
    6. 消息队列
        · 异步处理
        · 流量控制
        · 服务解耦
        · 消息可靠性:数据不丢失
        · Cluster:集群, 确保某个节点宕机不会导致服务不可用,消息也不能丢
        1. 生产者-> 消息队列
            这里可能丢失消息, 从生产者服务器发到消息队列服务器中,网络错误会丢包;网络抖动,可能导致发送超时,此时重试2-3次;这又会导致消息重复
        2. 消息队列中丢失消息
            队列没来得及消费,机器掉电了
            · 消费者和生产者模拟TCP,使用sequence序列号来判断消息是否丢失
                每次消费者收到的sequence都要+1才是对的
                · 分布式的情况下
                    在序列号的基础上,再加上指定分区号
            · 确保消息到达
                1. 生产者到消息队列采用ACK确认,如果不成功就重试
                2. 消息队列持久化,将消息存盘
                3. 消费者和消息队列也用ACK确认
            · 双工通信的消息顺序
                在消息里带上序列号
            · kafka
                · 提升网络IO性能
                    发消息时,buffer达到一定数量了,才一起发出去. 减少IO调用次数
                · 提升磁盘IO性能
                    顺序读写的性能远高于随机读写,持久化存储消息的时候,写满一个文件,就接着写一个新的文件
                · 读写缓存,读写比例1:1
                    · 利用PageCache加速读写
                        PageCache缓存磁盘上的文件,对应上面的write back策略
                · ZeroCopy 零拷贝技术
                    size_t sendfile(int out_fd/*限定这是网络套接字*/, int in_fd /*真实的文件*/, int offset, size_t count);
                    kafka使用这个套接字将持久化的文件发送给消费者
                · 数据的可靠性
                    在不同节点上的多副本来解决数据可靠性问题,这样即使某个服务器掉电丢失一部分文件内容,它也可以从其他节点上找到正确的数据,不会丢消息。
                · 保持缓存新鲜-只读缓存 读次数是写次数的几十倍
                    · 尽量让缓存中的数据与磁盘上的数据保持同步
                    · 定时读取磁盘到缓存中,更新缓存
                    · 设置缓存过期时间,过期了就更新一下
                · 缓存置换策略
                    · LRU
                        缓存要存那些数据
                    · LRU 2Q
                        在LRU的基础上增加-页面位置与尾部的距离这个条件。因为越是靠近尾部的数据,被访问的概率越大。

        3. 消费的过程中丢失
            网络抖动导致丢失或者异常导致消息没处理,这里如果丢失就不更新最后的结果就行
        4. 只消费一次(保证消息的生产和消费过程是幂等的)
            每条消息赋予一个唯一的ID,如果消息队列收到两个ID一致的就丢弃
            消费者每次收到ID,在数据库中查一下,如果已经存在就丢弃
        5. 减少消息延迟
        6. 分布式事务
            1. 开启事务->发送半消息(事务没提交之前,消费者看不到这个消息)到消息队列->在本地业务中创建订单,提交数据库事务->然后根据这个事务的执行结果决定提交还是回滚->成功了,消费者就可以看到这个消息并开始处理;否则看不到这个消息
                提交事务抛异常,需要重试提交直到成功或者删除这个订单

    7. RPC
        1. RPC框架
            · 数据收发
                

        2. 网络传输协议
            序列化/反序列化

}

;{    线程并发(同步互斥) C++并发编程实战 
        3.2 使用互斥量保护共享数据
            3.2.1 C++使用互斥量
                1. 使用std::mutex,提供lock/unlock接口,不提倡使用
                2. 优先使用RAII机制的模板std::lock_guard<Class T>
}

;{    select/epoll
}

;{    消息队列
    · zeroMQ解决传统网络编程的问题
        · 调用的socket接口较多;
        · TCP是一对一的连接;
        · 编程需要关注很多socket细节问题;
        · 不支持跨平台编程;
        · 需要自行处理分包、组包问题;
        · 流式传输时需处理粘包、半包问题;
        · 需自行处理网络异常,比如遭接异常中断、重连等;
        · 服务端和客户端启动有先后;
        · 自行处理IO模型
        · 自行实现消息的缓存;
        · 自行实现对消息的加密
    · SEND-ACK 发一次确认一次,吞吐量不高
}

;{    性能调优/Linux命令 极客时间·系统性能调优必知必会
    1. 基础设施优化
        1. cpu缓存 L1:32K(一个是数据缓存,一个是指令缓存,cpu独有) L2:256K(cpu独有) L3:20M(各个CPU共享)
            · 伪共享
                不同CPU上L1共享不同的变量,在一行Cache Line上有两个变量,本来是共享X变量,结果在一行上的Y变量也在共享行列中,这个Y就属于伪共享(不该共享,却被共享到了一行)
            · 缓存一致性
                · 缓存锁,对各个CPU中的共享缓存加锁
                · 当某个CPU的共享缓存被修改,就会通知其他CPU的cache无效
                · Store Buffers
                    · cpu0 修改L1缓存,需要在缓存一致性协议处通知其他CPU令其失效,然后返回来通知cpu0;于是引出Store Buffers
                    · CPU0 修改L1缓存,存入Store Buffers(如果后续有读取的,就从Store Buffer中读取),然后发消息通知其他CPU无效化,无需等待其他CPU回应(异步),就可以继续执行指令
                    · 等到其他的回应消息到达,再将store-buffer中的共享变量写入内存和其他cpu L1中
                    
            · cpu Cache Line, 和内存对齐,64字节
                CPU cache 读取一片buffer的时候,一次会读64个字节;4字节的数组在使用的时候,会把后面64个字节一起放到L1的一line上;如果不够64字节,就会顺序的补足后续元素
                // 4个字节的数组比8个字节的数组效率高就是因为4个字节附近的数据读取的多,cpu能快速访问
            · 按照内存布局顺序访问会带来性能提升
            · Nginx用Hash存储域名/http头部都是cache line大小的整数倍
                和MMU一样数据是cache line的整数倍,可以避免cpu多次读内存;比如数据长度50,CPU就要读取两次
            · perf
                perf stat 查看cpu cache命中率
            · 指令缓存
                for (i in 1, 100) if (num[i] < 128) num[i] = 0;
                sort(num);
                这里先排序再遍历 比 先遍历再排序要快
                · CPU分支预测器
                    if/switch语句至少可以分为2段语句,分支预测器会提前把预测好的指令放到指令缓存中,加快cpu运行,有序的数据,预测性能更好
                    · C/C++还提供预测分支概率工具, 帮助cpu预测
                        #define likely(x) __builtin_expect(!!(x), 1)
                        #define unlikely(x) __builtin_expect(!!(x), 0)
                        if (likely(a == 1)) //预测 很有可能 a==1
                        if (unlikely(a == 1)) //预测 不太可能 a==1
            · 提升多核cpu下的缓存命中率
                · 在cpu0 上运行的进程, 提前缓存的数据指令,因为进程切换到cpu1.那么上次的缓存就没用上.
                · sched_setaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask)
                    设置进程/线程绑定到指定cpu上
        2. 内存池
            · 堆的调用受并发影响, 可以考虑在栈上分配内存
        3. hash表管理亿级对象
            · 面对海量的分布式服务,使用更多的空间建立索引,获取更快的查询速度
            · Memcached/Redis都用哈希表管理数据
            · hash<bitset>只用于判断对象存不存在,布隆过滤器用来解决缓存穿透
            · 如果要查询指定元素附近元素,需要使用红黑树logN,内存不够,存储到磁盘上,用B+树
            · 内存结构和序列化方案
                1. 容灾
                    · 每隔一天把哈希表备份到另一台服务器上
                    · 服务器宕机, 先恢复隔天的备份数据恢复到hash表中,再通过log把一天内的数据载入到hash表,这就要求序列化和反序列化
                    · 开放寻址法
                        发生冲突的数据,依次在hash上平铺开来;然后使用mmap映射到文件中,msync刷新到磁盘;新的进程启动也可以重新映射恢复到hash表中
                    · hash序列化
                        1. hash装载因子接近1,就直接存盘
                        2. 装载因子小,空余的空间大
                            1. 数据的长度一致
                                一个数组D存数据,另一个hash数组H每个桶存放数组D的下标
                            2. 数据不一致
                                不同长度的分别分成一类数组用来存数据,哈希表数组每个桶分i+j位,用来记录不同的数组以及位置
                        3. 降低哈希表的冲突
                            1. 优化哈希函数
                                · 使用素数做数组大小
                                    合数做数组大小, 以因子个数递增, 有些key永远都不能击中
                            2. 扩容
                                老的哈希表需要重新插入到新哈希表中,分为后台多次迁移,提供服务的同时根据迁移情况选择新老哈希表
        4. 零拷贝
            · 磁盘拷贝到PageCache,直接发送到网卡缓冲区;减少系统调用,避免内核/应用进程切换
            · PageCache
                · 现在PageCahe中找,找不到就要阻塞读取磁盘了;如果找到了就返回
                · PageCache采用LRU算法,保留最近访问最多的,时间局部性;打开文件关闭后,再接着顺序读写磁盘,性能最高
                · 预读功能
                    类似CPU Cache, 读取0-32KB磁盘数据的时候会把读取指针之后的32-64KB也读取到PageCache中
                · 大文件占据PageCache,利用率低不应该使用PageCahe
            · 异步IO 只支持 直接IO(绕过PageCache直接发给应用buffer)
                · 高并发场景处理大文件,应当使用异步IO和直接IO来替换零拷贝技术
                · 读操作分两部分,前半部分向内核发起读请求,但不等待数据到位就立刻返回->此时可以做其他事->等IO完成后半部分,将数据拷贝到应用缓冲中->内核通知应用
                · 直接IO的场景
                    1. PageCahe已经缓存好磁盘数据了,不需要再次缓存
                    2. 高并发下传输大文件, 占用资源
                    3. 直接IO没法享受预读
        5. 协程
            · 零拷贝通过减少上下文切换(系统调用)次数提升文件传输的性能
            · 高并发也是通过降低切换成本实现
            · 多进程时间片轮换并发执行 缺点:
                · 内核管理成本高
                · 进程间内存同步不方便
                · Nginx为了稳定性使用多进程
            · 多线程
                · 共享进程内存
                · 多线程存在竞争问题
                · 多进程、多线程都难以实现高并发
                · 多线程在创建的时候,内存池还会预分配64M的内存,栈大小8M;一个线程就占了72M, 这就没有足够的线程并发开启几万个线程
                · 线程还受CPU核心的限制,最多8倍个线程
                · 线程切换
                    · 时间片用尽
                    · 线程被阻塞,内核为了让CPU充分工作,也会切换到其他线程执行
                    · 线程切换需要时间,线程数多的情况下,线程切换就会消耗大量的CPU时间
                    · 线程1----阻塞读---->内核---->磁盘1
                        切换到线程2----阻塞读---->内核---->磁盘2
                        线程1<---通知并返回--内核<---读盘写PageCache--磁盘1
                        · 现在要实现并发
                            1. 把上图中本来有内核实现的请求切换工作交给用户态来完成
                                1. 异步化编程通过应用层代码实现了请求切换,降低了切换成本和内存占用空间
                                2. 异步化编程依赖IO多路复用, 阻塞调用改异步调用,才能避免内核切换
                                    · 线程向epoll注册读写事件,epoll在读写就绪时返回通知线程进行读写;读写未就绪时可以做其他事
                                        · 这个异步化代码容易出错
                                            1. epoll负责监听
                                            2. 应用自己负责读写
                                            3. 这违背了内聚性原则, epoll的监听和应用的读写分离
            · 协程是如何实现高并发的
                1. 和异步编程相似的地方
                    · 使用非阻塞的系统调用和内核交互,切换请求由用户完成
                2. 不同的地方
                    · 协程把异步化的两段函数(epoll,用户读写),封装成一个阻塞的协程函数
                        · 协程发起读请求,这个协程函数发出异步读,数据未就绪先切换到其他协程,等到数据就绪后,再切换回来
                3. 协程栈 用户态的线程
                    1. 从进程的堆中分配,线程栈有8M,协程栈一般几十K,C库内存池也不预分配
                    2. 协程必须重新封装所有的阻塞系统调用,否则一旦线程切换,这个线程就会进入休眠,整个线程上的协程都得不到执行
                        · 自己实现sleep,互斥锁等
                    3. 设置该协程所在线程的优先级,绑定CPU(增加CPU缓存命中)
        6. 平均负载
            · uptime(1,、5、15分钟内的平均负载)
                单位时间内,系统处于可运行状态和不可中断状态的平均进程数,也就是平均活跃进程数
                · CPU密集型会导致平均负载升高
                · IO密集型,等待状态下也会导致升高
            · stress -c(进程数) --timeout(运行时间) 600
            · mpstat -P ALL 5 查看所有CPU的运行状态
            · pidstat -u 5 1 查看所有进程运行状态
        7. 上下文切换
            · vmstat -5可以查看线程上下文切换次数,CPU使用率,中断次数
            · pidstat -wt 1 可以看到具体进程CPU使用率,线程上下文切换次数
            · /proc/interrupts 
                watch -d “cat /proc/interrupts|tail -20” 查看所有CPU某种中断的次数
                Rescheduling interrupts表示调度中断
        8. cpu占用率100%
            // # -g 开启调用关系分析,-p 指定 php-fpm 的进程号 21515
            $ perf top -g -p 21515 查看占用率高的函数栈
                perf top以top的形式显示各种事件,默认cpu cycle
                -g 显示调用堆栈、 -p指定进程
            


    2. 系统层网络优化
        1. 性能好,效率高的一对多通讯该如何实现？
            1. 广播
                1 应用层广播一般不用IP层,选择UDP协议广播
                    广播交由交换机分发,设置SO_BROADCAST选项
                    1 只对子网广播,将目标IP设为255.255.255.255,交换机就会广播
                    2 多个子网情况,将子网主机段设为11111
                    3 路由器默认不转发广播报文,避免网络风暴
            2. 组播
                D类地址,组播前四位必须是1110,IP地址范围从224.0.0.0到239.255.255.255;通过IGMP协议,将IP地址加入虚拟组中,setsocket就可以操作IGMP协议管理组播地址
                    · 使用IP_ADD_MEMBERSHIP就能向虚拟组中添加IP,IP_DROP_MEMBERSSHIP从组中去除某个主机IP
        2. C10M
            · 什么样的代码值得基于事件来做拆分呢？还得回到高性能这个最终目标上来。我们知道,做性能优化一定要找出性能瓶颈,针对瓶颈做优化性价比才最高。对于服务器来说,对最慢的操作做异步化改造,才能值回开发效率的损失。而服务里对资源的操作速度由快到慢,依次是 CPU、内存、磁盘和网络。CPU 和内存的执行速度都是纳秒级的,无须考虑事件驱动,而磁盘和网络都可以采用事件驱动的异步方式处理。
                //磁盘和网络采用异步方式
                · 相对而言,网络不只速度慢,而且波动很大,既受制于连接对端的性能,也受制于网络传输路径。把操作网络的同步 API 改为事件驱动的异步 API 收益最大。而磁盘(特别是机械硬盘)访问速度虽然不快,但它最慢时也不过几十毫秒,是可控的。而且目前磁盘异步 IO 技术还不成熟,它绕过了 PageCache 性能损失很大。所以当下的事件驱动,主要就是指网络事件。
                    //磁盘读写虽然慢,但可控,所以主要针对网罗
            · 该怎样处理网络
                · select每次轮询的时候把套接字复制到内核缓冲区查询,然而要及时的处理网络IO就要每隔几十毫秒就要收集一次,性能消耗巨大
                · epoll分两步来处理
                    1. epoll_ctl将套接字传给内核
                    2. epoll_wait就不用再传递套接字了,把轮询时候重复传递改成只传一次,降低性能损耗
                        · 千兆网卡,每秒接收100MB数据,每个请求10KB,那么每秒就是1万个请求、10万个事件需要处理。
                        · 每隔100毫秒epoll_wait轮询一次,每次就是1万个事件需要处理
                        · 只要保证一个事件的平均时间小于10微妙(多核处理器可以做到),100毫秒内就可以处理完这10000个事件
                        · 上面就做到了每秒1万次请求处理,这就是epoll在C10M下实现高吞吐量的原因
                    3. 收到socket后,处理任何一个事件的耗时应该都是微妙或者毫秒级的,否则就会延误其他事件处理,因为100ms后就要轮询新的事件了,为了并发处理,就要压缩处理时间
                    4. 压缩处理时间
                        1. 计算任务
                            虽然CPU和内存的速度很快,但是循环执行也可能耗时达到秒级,所以一定要引入密集计算才能完成的请求,为了不阻碍其他事件的处理,1. 放入独立的线程异步完成 2.分段完成,每次间隙的完成
                        2. 读写磁盘
                            · 写磁盘,使用了write back策略,write将应用缓冲的数据拷贝到PageCache并标记为Dirty返回,由内核择机将数据写进磁盘,所以这个写磁盘会很快
                            · 读磁盘,读取比较慢的情况下,将大文件的读取分成多分,每次只读几十K,降低单词操作的耗时
                        3. 网络访问
                            将套接字设置为non_block模式
        3. 提升TCP三次握手的性能
            1. 三次握手占据一个HTTP请求的平均时间在10%以上,网络不佳、高并发或者SYN洪泛攻击下,如果不正确调整三次握手的参数就会对性能有很大的影响
            2. 客户端优化
                1. SYN_SENT
                    客户端发出SYN;如果没收到ACK报文,就会重试,重试次数由/proc/sys/net/ipv4/tcp_syn_retries参数控制 默认6次
                    1. 1s 2. 2s 3. 4s 4. 8s...依次重试,直到最后依然没有收到ACK,就会终止三次握手,总耗时超过2分钟
                        · 内网通信一般不会长时间不回ACK报文,所以可以调低重试次数
            3. 服务端优化
                1. SYN_RECV
                    服务端对客户端发出SYN+ACK后成SYN_RECV状态,服务端有一个SYN半连接队列来维护这个未完成的握手信息,这个队列溢满后,就不会有新的连接进来,所以扩大并发就要扩大这个队列
                        · SYN半连接队列
                            /proc/sys/net/ipv4/tcp_max_syn_backlog 默认值1024 允许同时1024个套接字等待三次握手
                            因为超出1024个失败的,netstat -ano |grep "SYNs to LISTEN"
                                这个可以查找到因为满1024被drop掉的连接次数
                            · syncookies
                                cat /proc/sys/net/ipv4/tcp_syncookies 默认值1
                                0 表示关闭,1表示半连接队列满了,开启,2表示无条件开启;一般用1
                                开启后,半连接队列满了也可以建立连接
                            · ESTABLISHED
                                客户端回复ACK后就成为ESTABLISHED,服务端收到ACK才变成ESTABLISHED;如果没有收到就会一直重发SYN+ACK报文,当网络繁忙,丢包就会严重,此时应该调大重发次数
                                    ·cat /proc/sys/net/ipv4/tcp_synack_retries 默认值5 
                                        和客户端的SYN重发类似;收到客户端的ACK后,内核就从半连接队列中移到accept连接队列中,等待accept取出;如果accept不能及时取出造成accept队列溢出,就会丢掉这个建立好的TCP连接
                                    · cat /proc/sys/net/ipv4/tcp_abort_on_overflow 默认值0
                                        1开启这个功能,只要服务器收到ACK但不回复,客户端就会一直重发;等到accept队列有空就可以进来;只有你非常确定accept队列会长期溢出时,才设置1开启
                                    · accept队列长度
                                        1. listen的backlog就可以设置,backlog等于半连接+连接的个数
                                        2. backlog参数还受限于linux系统级的队列长度上限,一个端口监听的最大队列长度 4Kb
                                            ·  cat /proc/sys/net/core/somaxconn 默认值4096
                                        3. 查看accept连接队列溢出失败个数
                                            netstat -s|grep "listen queue"
                                        4. 调大backlog以及somaxconn
                2 cat /proc/sys/net/ipv4/tcp_fastopen 默认1
                    1. 表示作为客户端支持tcp_fastopen
                    2. 表示作为服务器支持tcp_fastopen
                    3. 都支持
                    4. tcp_fastopen在第一次三次握手,从服务器下载一个aes加密的cookie;第二次之后的连接,就可以用这个cookie加到SYN报文中发送到服务器,在SYN报文中添加请求数据
                        因为不用建立连接,所以用sendmsg/sendto 向指定的IP套接字发送请求
        4. 提升tcp四次挥手的性能
            · 先关闭连接的一方叫主动方,后关闭连接的叫被动方,一般服务器会是主动关闭连接的一方
            · 挥手过程
                1. 主动关闭连接的时候,被动方仍然可以在不调用close的状态下,长时间发送数据,此时处于半关闭状态(主动方关闭,被动方还在)。
                2. FIN表示关闭所在端的写入,主动方会进入FIN_WAIT1阶段,等待被动方ACK,被动方内核收到FIN就会回ACK,进入CLOSE_WAIT(所在端还没关闭)。被动收到ACK就进入FIN_WAIT2,等待被动方也关闭。
                3. 被动方在收到FIN,进入WAIT_CLOSE时,read返回0说明主动端发了FIN。被动方就针对性的调用close,进而发送FIN给主动端,此时被动方进入LAST_ACK,等待主动端的ACK。主动端在发送ACK之后,就进入CLOSE_WAIT,等待TIME_WAIT时间后就彻底关闭(CLOSED)。被动方收到ACK就彻底关闭(CLOSED)
            · 主动方的优化
                · 孤儿连接 orphan-conn
                    close关闭读写,对方在半关闭状态下发送的数据,这时也读不到
                    shutdown函数调用后,进入FIN_WAIT1或者FIN_WAIT2,仍然可以读到数据
                · FIN_WAIT1->FIN_WAIT2
                    一般只要数十毫秒(收到对方的ACK),只有迟迟没收到才可以用netstat看到FIN_WAIT1状态,此时内核会重发FIN报文,重发次数由tcp_orphan_retries参数控制
                    · cat /proc/sys/net/ipv4/tcp_orphan_retries 默认值0表示8次
                        · 控制FIN报文的重发
                        · 如果FIN_WAIT1状态比较多,说明重发的FIN包多,这时候要降低tcp_orphan_retries次数,到达次数后连接就会直接关闭CLOSED
                        · 一直发不出去可能是缓冲区还有数据还没发完;也有可能是对方在下载大文件时的接收窗口为0,这时候FIN就发不出去了
                        · cat /proc/sys/net/ipv4/tcp_max_orphans 默认值65536
                            close后的孤儿连接,超过这个值,就不再走四次挥手,而是发RST报文强制关闭连接
                    · cat /proc/sys/net/ipv4/tcp_fin_timeout 默认值60
                        主动方close进入FIN_WAIT2的持续时间不能太长,孤儿连接如果60s没收到FIN报文,就会直接关闭进入CLOSED
                · TIME_WAIT
                    主动方收到被动方的FIN报文后回复ACK,等待60s后关闭
                    · MSL (Maximum Segment LifeTime 报文最长生存时间)30s
                    设置为2MSL,可以保证在1MSL内丢包,2MSL重新收到;如果2MSL也丢失那么这个网络环境已经坏掉了
                    · 端口占用 TIME_WAIT占用不少资源
                        · cat /proc/sys/net/ipv4/tcp_max_tw_buckets 65536
                            当TIME_WAIT的连接数量超过该参数时,新关闭的连接就不再经历TIME_WAIT而直接关闭
                        · 当服务器的并发连接增多时,同时处于TIME_WAIT状态的连接数量也会变多,此时应该调大tcp_max_tw_buckets,减少不同连接间数据错乱的概率
                    · tcp_max_tw_buckets也不能太大,毕竟内存和端口号都是有限的
                    · cat /proc/sys/net/ipv4/tcp_tw_reuse 2
                        · 1 表示开启,允许在安全条件下使用TIME_WAIT状态的端口
                        · cat /proc/sys/net/ipv4/tcp_timestamps 
                            · 客户端和服务端都要打开这个,在配合上面的参数一起用
            · 被动方的优化
                · 被动方收到FIN报文后,开始四次挥手流程,发出ACK后,进入CLOSE_WAIT状态,等待close调用
                · linux没有限制CLOSE_WAIT状态的持续时间,对方如果用shutdown关闭,说明他还想在半关闭连接上收数据
                · 当然,大多数应用程序并不使用 shutdown 函数关闭连接,所以,当你用 netstat 命令发现大量 CLOSE_WAIT 状态时,要么是程序出现了 Bug,read 函数返回 0 时忘记调用 close 函数关闭连接,要么就是程序负载太高,close 函数所在的回调函数被延迟执行了。此时,我们应当在应用代码层面解决问题。
                · 在调用close后,发出FIN报文就进入LAST_ACK状态了,等待ACK回复,如果迟迟收不到ACK,内核就会重发FIN报文,重发次数由tcp_orphan_retries控制
                · 如果同时发送FIN报文,都进入FIN_WAIT,结果在等ACK的过程中都等来了FIN报文,连接双方就会进入CLOSING状态,最终双方等不到ACK,就会一直重发FIN报文,最终关闭连接
        5 如何修改TCP缓冲区才能兼顾并发数量与传输速度？
            · C10K 到 C10M后,TCP占用的内存翻了1000倍,这也意味着TCP占用的内存翻了1000倍,服务器的内存资源会非常紧张
            · free命令
                · buff/cache 缓冲/缓存
                    · 他是内核内存,与进程无关
                    进程每新建一个TCP连接,buff/cache就会上升4K,当连接传输数据时,就远不止增加4K。几十万并发就在进程内存外增加了上G内存消耗
                        · 这是因为TCP连接是由内核维护的,内核要为连接建立内存缓冲。太大了耗内存,太少了,无法充分使用网络带宽
            · 滑动窗口是怎样影响传输速度的
                · 发送报文没有收到ACK时,一段时间超时后,就会尝试重发,这时候,套接字缓冲区的报文还不能清空,重发还要用(内核buff/cache会增长的原因)
                · 发送一个报文再等一个ACK,在发送
                    发送一个报文的时间为RTT,一个1kb的报文需要10ms的话1kb/10ms=100K/s,可见这种确认报文方式太影响传输速度了
                    · 提速方式
                        · 并行的批量发送报文,再批量确认报文,MSS=1KB,那么就是10万个报文,等他们慢慢ack。并发的发送100MB文件,发送的瞬间速度就是100MB/10ms = 10G/s
                · 滑动窗口大小在TCP报文窗口字段只有2个字节大小,也就是2^16 = 65535大小,0表示窗口关闭。那么这样计算,65K/10ms = 6MB/s,这显然不够
                    · cat /proc/sys/net/ipv4/tcp_window_scaling 1
                        扩充窗口设置1表示窗口的最大值可以达到1GB
                        · 这样看来,进程的发送和接收缓冲也能足够大,那么传输量就会很大;但路由器不支持,超过了就会丢弃
                · 带宽时延积如何确定最大传输速度
                    · 100MB/s宽带,10ms内传输1MB个字节,TCP窗口大小就不能超过这个值
                · 缓冲区动态调节功能
                    cat /proc/sys/net/ipv4/tcp_wmem //4096	16384	4194304
                        1. 发送缓冲区 4096代表动态的窗口下限,4194304代表上限,16384代表初始默认值
                    cat /proc/sys/net/ipv4/tcp_rmem  //4096	131072	6291456
                        1. 接收缓冲区同上
                        2. 依据空闲系统内存的数量来调节接收窗口
                             · cat /proc/sys/net/ipv4/tcp_moderate_rcvbuf //1
                                开启接收缓冲区的调节功能 1表示开启
                                cat /proc/sys/net/ipv4/tcp_mem //117480	156640	234960 
                                    · 这是页面大小,表示4KB一个
                                    · 当TCP内存小于第一个值,不需要自动调节;在1,2之间,内核开始调节接收缓冲区的大小,大于第三个值,内核不再为TCP分配新内存,此时无法建立连接
                                    · 通过内核设置TCP缓冲区的动态调整功能,而不要在socket上直接设置SO_SNDBUF或者SO_RCVBUF
        6 如何调整TCP拥塞控制的性能？
            1. 慢启动阶段如何调整初始拥塞窗口
                · 查看初始拥塞窗口大小
                     ss -nli|grep cwnd //cubic cwnd:10 
                · 修改初始拥塞窗口
                    ip route | while read r; do
                        ip route change $r initcwnd 10;
                    done
                    更大的初始拥塞窗口,更快的遭遇网络阻塞,更快的结束慢启动阶段
            2 慢启动的结束
                1. 定时器探测到丢包,等待超时了再重传
                2. 拥塞窗口达到慢启动阈值ssthresh(slow start thresh)
                3. 3次ack报文,快速重传,比超时快
                4. 1,3场景,建议加大ssthresh值,快速发现网络瓶颈
                5. 结束慢启动进入拥塞避免,用线性速度慢慢增加拥塞窗口
            3. 快速恢复
                出现三次ACK丢包后,进入快速恢复阶段,ssthresh变为一半,拥塞窗口变成一半;直到收到丢失的包之后才会进入拥塞避免阶段
                · 查看内核支持的congestion拥塞算法
                    cat /proc/sys/net/ipv4/tcp_available_congestion_control //reno cubic
                · 设置congestion算法
                    cat /proc/sys/net/ipv4/tcp_congestion_control //cubic
            4. 基于测量的拥塞控制算法 congestion control
                · 当开始出现丢包时,拥塞已经非常严重了;进行拥塞控制的最佳时间点,是缓冲队列刚出现积压的时刻,此时,网络时延会增高,但带宽维持不变,这两个数值的变化可以给出明确的拥塞信号
                · 这种测量带宽、时延的方法被称为BBR算法
                    cat /proc/sys/net/ipv4/tcp_congestion_control = BBR
        7 单机如何实现管理百万主机的心跳服务
            · 类似2MSL:考虑到网络报文的延迟,如果管理服务在几个上报周期内未收到心跳,则认为主机宕机
            · 扩大端口范围
                cat /proc/sys/net/ipv4/ip_local_port_range //32768	60999
            · 定时向客户端探测连接是否存活
                cat /proc/sys/net/ipv4/tcp_keepalive_time //7200
            · 每次探测的最大等待时间
                cat /proc/sys/net/ipv4/tcp_keepalive_intvl //75
            · 保活重试次数
                cat /proc/sys/net/ipv4/tcp_keepalive_probes //9
            · 网络协议的选择
                超过MTU选择TCP,TCP解决了流量控制、可靠性
                小于MTU选择UDP,不用建立连接

    3. 应用层编码优化
    4. 分布式系统优化
}

;{    ubus网络数据协议
    |blob_buf|
    |blob_buf.buf|
        |
    |blob_attr.id_len(记录整个buf的大小) +blob_attr.data(0)|-------------------------------------------------------------|
                                                          |blob_attr.idlen+blob_attr.data|blob_attr.idlen+blob_attr.data|
                                                                                    |
                                                                            |blobmsg_hdr.name:value|
        · blob_attr{ 
            uint32_t id_len; // id高一个字节存储数据类型,len低三个字节;
            char data[]; 
        } //还要对blob_attr对齐
            · 对齐算法
                (len + ALIGNMENT - 1) & ~( ALIGNMENT - 1 ) // a - 1 + n / a = 0, 1, 2 ··· a - 1 , 0, 1 ... a + 1;  n == 1, 1+a, 1+2a才增长
        · blob_buf {
            struct blob_attr *head; // 指向blob_buff的整个空间
            bool (*grow)(struct blob_buf *buf, int minlen); // 控制blob_buf内存自动扩充,每次增加256个字节
            int buflen; // blob_buf长度
            void *buf; // 指向一块用blob_attr填充的空间
        }
        · blob_buf_init // 初始化blob_buf, 控制自增,给buf填充第一个bob_attr空间
        · blobmsg
            · blobmsg_json用于序列化到json
            · blobmsg_add_u32(&b, "id", 123);
                从前面初始化的blob_buff的第一个attr后面,增加一个attr的"id"-123的字符串和数字的大小,设置第一个attr的大小加上新加的数据
                struct blobmsg_hdr {
                    uint16_t namelen;
                    uint8_t name[];
                } __packed;
                blob_attr的data用blobmsg_hdr填充name字段,偏移namelen后,填充123这个value
    · protobuf
        · 四个字节的整数序列化只需要2个字节存储,可以减小传输的数据大小
}

;{    Linux高性能服务器编程 pdf
    8 高性能服务器程序框架
        8.2 服务器编程框架
            · IO处理单元
            =======消息队列=====
            · 逻辑单元
            =======消息队列=====
            · 存储单元
        8.3 I/O模型
            · 非阻塞IO
                · I/O复用
                    · select/poll/epoll_wait注册IO触发事件
                · SIGIO
                    · fcntl(int fd, SIGIO, void*args) // fd触发的时候,会收到SIGIO信号
        8.4 事件处理模式
            · Reactor
                · 主线程负责listen事件,工作线程自己从socket上读取客户请求并往socket写入服务器应答
                · 连接数(百万)
                    1. ulimit open-files = 1 000 000
                        sysctl -p 生效
                    2. nf_conntrack_max nf_conntrack模块在跟踪连接的时候需要创建一个hash表存储连接, 太小了,导致新来的连接存不了
                    3. 连接耗时: 扩大listen队列 accept并发
                · IO处理单元
                    主线程epoll注册listenfd连接,epoll_wait触发就将消息加入到消息队列
                =======消息队列=====
                    工作线程处理请求epoll注册写事件,主线程epoll_wait等待可写事件,将可写事件加入队列
                =======消息队列=====
                · 逻辑单元
                    工作线程往fd发送消息
            · Proactor
                1. 主线程和内核完成所有的IO操作,工作线程仅仅负责业务逻辑
                2. aio_read注册读完成事件
                3. 数据到达,触发信号处理函数,选择一个工作线程处理客户请求, aio_write注册写完成事件
                4. 数据发送完成,内核发送信号,触发信号处理函数,选择一个工作线程来做善后,比如决定是否关闭socket
            · Proactor将IO处理单元完全隔离开
        8.5 两种高效的并发模式
            · 计算密集型,并发编程还会因为任务切换降低效率
            · IO密集型,可以在IO阻塞时让出CPU
            1. 半同步/半异步
                IO读取数据,添加请求到消息队列是异步的;工作线程从队列中取数据是同步的
                · 变种1 消息队列插入的是连接socket,工作线程自己读写数据,就变成半同步-半reactor了
                    两个缺点:
                        · 消息队列使用互斥锁浪费等待时间
                        · 连接socket过多,工作线程消费慢,造成积压
                · 变种2 一种高效的模式,将消息队列改成管道
                    主线程通过管道传递连接socket是独立的,工作线程使用epoll_wait监听管道是同步,工作线程半异步处理数据
    
    13 多进程编程
        13.1 fork系统调用
            · fork复制进程映像, 虚拟空间上的数据区、代码区、堆/栈区都一样
                · 内核进程表中会新加一个进程表项,ppid指向父进程
                · 新建进程注册的信号位图被清空,不再有注册的信号处理
            · 这里的复制,遵循COW(写时拷贝)
            · 文件描述符也被拷贝,引用计数+1,并且父进程中打开的,在子进程中也是打开的
        13.2 exec系统调用
            · 替换当前进程映像,代码段等被覆盖,从exec之后就在新的进程中执行
            · exec是阻塞函数
            · 父进程打开的文件描述符不会关闭, 除非设置SOCK_CLOEXEC
}

;{    base64编码
        每3个字节24位, 按每6位转换成4个值在0-63之间的字符
        16: 0x30 0x82 0x02
        2: 00110000 10000010 00000010
        6bit: 001100 001000 001000 000010
                12      8      8     2
        BASE64: M       I      I     C
        · 数据不满24位的, 不够3字节的补齐,用=号
}

;{    九章算法
    1. 如何快速得到子数组的和(双指针、前缀和数组)
        1. 前缀和数组
            1. O(n^2)
                · num[10] = {1,2,3,4,5,6,7,8,9,10};
                · 构造前缀和数组
                int sum = 0;
                    PrefixSum[i] = for_each(&num[0], &num[i], [](int x){
                        return sum += x; });
                · 判断某个子数组和>=k满足条件
                    for (i in start, end) //从头到尾一个个遍历
                        for (j in i, end) // 一次从每个数开始计算一个子数组内符合的条件
                            sum_i_j = Prefixsum[j] - Prefixsum[i];
                            if sum_i_j >= k; then return "found";
            2. O(n * log(n))
                · 在前缀和的基础上优化
                · for (i in start, end) //从头开始遍历
                  {
                    while(left + 1 < right) // 二分查找前缀和数组
                    {
                        mid = left + (right - left) / 2; // base下标 + 个数
                        if (PrefixSum[mid] - PrefixSum[start] >= k) // [start, mid]数组和>=k说明目标在mid左边;否则在mid右边
                        {
                            right = mid;
                        }else
                            left = mid;
                    }
                    if (PrefixSum[mid] - PrefixSum[start] >= k) // 查找到最后的值,比较下是哪个
                        return left;
                    return right;
                  }
        2. 双指针(全是O(n), 同向,想向,背向)
            1. 同向
                · 特点: 指针不回头
                全零字串数量
                · 算法模板
                    int sum = 0;
                    num[10] = {1,2,3,4,5,6,7,8,9,10};
                    for i in start, end // 从头到尾遍历,i指针后移
                    {
                        while(j < n && sum < k) // 比较条件, 总的执行次数是N
                        {
                            sum += num[i];
                            j++; // j指针后移
                        }
                        sum -= num[i];
                    }
            2. 相向
                两数之和、三数之和
            3. 背向
                最长回文子串
    2. 买卖股票的最佳时机(隔板法)
        · price[8] = [4, 4, 6, 1, 1, 4, 2, 5];
            SumMaxProfit = 0;
            for (i in start, end) // 遍历安插隔板
            {
                int LeftMaxProfit = GetMaxProfit(0, i);
                int RightMaxProfit = GetMaxProfit(i, end);
                SumMaxProfit = std::max(SumMaxProfit, LeftMaxProfit+RightMaxProfit);
            }

            GetMaxProfit(start, end) // 单独计算每个隔板内的最大利润
            {
                int MinPrice = INT_MAX;
                int MaxProfit = 0;
                for (i in start, end){
                    MinPrice = std::min(MinPrice, price[i]);
                    MaxProfit = std::max(MaxProfit, price[i] - MinPrice);
                }
                return MaxProfit;
            }
    3. BFS(Breath First Search)模板
        1. 在图中找到从起点start到终点end的最近距离
            Queue<Node*> queue; // 取出每一层的一个节点,把相邻节点添加到队列后
            Set<Node> visited; // 已经遍历过的不用再访问
            queue.push(start); //初始化起点
            visited.push(start); //标记起点
            int step = 0;

            while (!queue.empty())
            {
                int sz = queue.size(); //获取当前层的节点个数
                for (size_t i = 0; i < queue.size; i++)
                {
                    Node *cur = queue.pop();
                    if (cur == end)
                        return step;

                    for (tmp in cur.next) // 遍历节点的邻接点
                    {
                        if (visited.find(tmp) == visited.end()) // 避免循环遍历
                        {
                            queue.push(tmp);// 添加下一层节点
                            visited.insert(tmp);
                        }
                    }
                }
                ++step;
            }
        1. 连通块
        2. 分层遍历
        3. 拓扑排序
    4. DFS(Depth First Search) 深度优先 又称回溯算法
        1. 模板
            · 路径: 已经做出的选择
            · 选择列表: 当前可以做的选择
            · 结束条件
            result = [] // 记录做出的选择
            def backtrack(路径):
                if 满足条件:
                    result.add(路径)
                    return
                
                for i in 选择列表:
                    做选择 // 选择一条路径,开始往路径添加节点
                    backtrack(路径)
                    撤销选择 //开始第二条路径
        
        1. 全排列问题
            · nums[] = {1,2,3};
            · List<List<int>> liliPermu;// N个全排列
            permute(){
                List<int> liPermu;
                backtrack(liPermu);
            }

            backtrack(liPermu)
            {
                if (liPermu.size == ARRAYSIZEOf(nums)) // 结束条件,一个全排列的长度排满
                    liliPermu.push(new List<int>(liPermu)) // 塞入一个全排列结果

                for (int i = 0; i < ARRAYSIZEOf(nums); ++i){
                    if (std::find(num[i]) == liPermu.end()) // 选择路径,
                        liPermu.push_back(num[i]);

                    backtrack(nums, liPermu);
                    liPermu.pop_back()
                }
            }
    5. 字典树(Trie)
        · 由边和节点构成的字典树,优化字符串的公共前缀
        · 数据结构
            class TrieNode{
                public:
                    Map<char, TrieNode> children; //每个子节点存储字符到TrieNode的Map
                    bool IsWord; // 指示某个子节点是不是一个word或者说是叶子节点
                    String Word; // 叶子节点存储一个字符串
                    TrieNode(){
                        IsWord = false;
                        word = NULL;
                    }
            };

            void insert(char word[]) // 往字典树中插入一个字符串
            {
                TrieNode *node = root;// 获得根节点
                for (size_t i = 0; i < word.length; i++) // 遍历字符串
                {
                    if (node->children.find(word[i]) == node->children.end())
                    { // 检查一个边为word[i]的节点是否存在
                        node->children.insert(make_pair<char, TrieNode>(word[i], new TrieNode()));
                        // 给root节点创建一个边
                    }
                    node = node->children[word[i]]; // 遍历到字符串的下一个字符
                }
                node.isword = true;
                node.word = word; //给叶子节点设置字符串
            }
    6. 最大/小堆(用顺序结构存储二叉树)
        · 数组下标0不用, 1为root, 左节点在2n处, 右节点在2n+1处,父节点在n/2处
        · 最大/小堆只保证堆顶比子节点大或者小,不保证左右子节点左小右大
        · 插入时,将值放在数组尾部空处, arr[N] = insert_value;然后向上浮动到正确的位置
            cur = N;
            while (cur > 1 && arr[N] < arr[N/2]) //插入N处的值,和父节点作比较
                exch(arr[N], arr[N/2]), cur = N/2; // 不停的上浮到合适的位置
        · 删除时,交换arr[1](堆顶,删除就是pop堆顶)和队尾值arr[N], 然后将交换后的值从堆顶下沉到正确的位置
            exch(arr[1], arr[N]); // 交换堆首/尾,删掉队尾
            cur = 1;
            do{
                if ()
                    break; // 这就是最小的
                if (left[cur] < right[cur]) // 从子结点中选一个小的和父节点作比较,保证是最小堆并下沉
                {
                    exch(arr[cur],left[cur]); // 交换父子节点
                    cur = 2 * cur; // 下沉父节点
                }else{
                    exch(arr[cur],right[cur]); // 交换父子节点
                    cur = 2 * cur + 1; // 下沉父节点
                }
            }while( !(arr[cur] < left[cur] && arr[cur] < right[cur]) ); 
            // 如果小于子节点,就找到合适的位置了



}

;{    IO函数 UNIX网络编程卷1
    1. recv(int fd, void*recvbuf, size_t recvlen, int flags)
        · flags 经常使用MSG_DONTWAIT // 设置IO操作为非阻塞,执行完,关闭非阻塞标志
        · return value;
            · >0 这次IO读取到的字节数
            · 0, 连接对方正常关闭连接
            · -1, 代表发生错误,errno可以获取错误码
                EAGAIN 套接字为非阻塞模式, IO却是阻塞模式; 超时时间内没有收到数据
                EINTR 接收到中断信号
        · 和其他IO函数一样,都是从内核接收缓冲区,复制到用户空间; 驱动设备还要从驱动缓冲区拷贝到内核(这叫2次拷贝)
        · 应用缓冲区比内核小: 需要循环调用IO函数直到读取完所有数据
        · 应用缓冲区比内核大: 一次可以读取所有数据
        · 应用读取到的时候,内核缓冲区就会丢弃这些数据
    2. send(int fd, void*sendbuf, size_t sendlen, int flags)
        · return value;
            · >0 这次IO发送的字节数
                发送的数据长度比实际发送长, 需要循环发送,直到发送完成;发送缓冲区是通过移动偏移指针,指向发送的位置
            · 0, 连接对方正常关闭连接
            · -1, 通过errno查看错误原因
                EAGAIN/EWOULDBLOCK 套接字为非阻塞模式,IO却是阻塞;发送缓冲区的数据在超时时间内没有发送出去
                EPIPE 套接字已经关闭
                EACCES 套接字不可写
                ENOBUFS 发送缓冲区已满
    3. readv(int fd, const struct iovec*vector, int count);
        · struct iovec{
            void *iov_base; // 如下,指向buffer地址
            size_t iov_len; // 指明使用的buffer长度
        };

        char arr[30];
        struct iovec vecIo[3] = {{.iov_base=&arr[0], .iov_len = 10}};
        vecIo[1].iov_base = &arr[10];
        vecIo[1].iov_len = 10;
        vecIo[2].iov_base = &arr[20];
        vecIo[2].iov_len = 10;


    4. writev(int fd, const struct iovec*vector, int count); // 同readv
    5. recvmsg(int fd, struct msghdr *msg, int flags);
        · 辅助数据
            struct cmsghdr {
                socklen_t        cmsg_len;
                int                  cmsg_level;
                int                  cmsg_type;
            };

        struct msghdr{
            void *msg_name; // 源地址,指向struct sockaddr的指针
            socklen_t msg_namelen;
            struct iovec *msg_iov;  //如上
            size_t msg_iovlen;  // msg_iov的个数
            void *msg_control;  // 指向cmsghdr控制信息
            socklen_t msg_controllen;
            int msg_flags;
        };
        · 该函数从内核的接收缓冲区复制数据到应用缓冲
            · 当内核中的数据比指定的缓冲区小时,会复制缓冲区中的所有数据到用户缓冲区,并返回数据的长度。
            · 当内核接收缓冲区中的数据比用户指定的多时,会将用户指定长度len的接收缓冲区中的数据复制到用户指定地址,其余的数据需要下次调用接收函数的时候在复制
        · udp无连接套接字收到的第一个msg_name是对方原地址

    6. sendmsg(int fd, struct msghdr *msg, int flags); // 同上 
    7. read/write可以对所有文件描述符使用
        recv/send,recvfrom/sendto,recvmsg/sendmsg只能操作套接字
        readv/writev可以使用多个buffer
        recvfrom/sendto可以指定对方IP套接字
        recvmsg/sendmsg可以在struct msghdr中指定对方IP和控制信息,
            sendmsg用flags参数控制socket MSG_NOWAIT
            recvmsg用msg_flags参数控制
    8. mmap
        · 先分配虚拟内存,然后做虚拟内存-物理内存映射
        · mmap映射文件,先分配虚拟内存,然后将虚拟内存映射到pagecache的物理地址上。读数据的时候,读到就直接返回;没读到,就会读磁盘,再复制到pagecache,也就是返回mmap映射的虚拟内存上

}

;{    进程间通信 Linux内核设计的艺术:图解Linux操作系统架构设计与实现原理 第2版
    1. pipe管道
        · 内核给每个管道分配一页内存,并给予文件属性,即具有读写接口.//进程的PCB的files指针会记录,并创建inode节点, linux默认大小65535
        · 这一页内存分配在内核空间,即不属于任何用户进程,并与inode节点关联起来.//读写的时候意味着进入内核空间
        · 因为分读写端,所以创建两个文件描述符fd,同时指向inode节点指向的内存
        · fd[0]设置为读属性, fd[1]设置为写属性
        · 读管道时,没有可读数据时,sleep_on就会挂起,加入等待队列,schedule到其他进程;有数据时,wakeup,从等待队列中唤醒
        · 写管道类似, 条件时有无可写空间
    2. socketpair 双向管道,线程间通信

    3. signal信号
        · signal底层由sigaction实现
            · 可不可靠与安装函数无关,只与信号量有关,sigaction可以传递信息,signal不行
            · task_struct结构中signal有关于本进程中未决信号的数据成员
        · struct sigpending pending
                struct sigqueue *head, *tail; // 存储消息信息
                sigset_t signal;    // 信号集,位图存储信号集64个信号SIGMAX
            };
            · struct sigqueue{
                        struct sigqueue *next; // 维护一个队列
                        siginfo_t info; // 用来存储kill发来的消息信息
            }
                · 每次注册消息的时候,就给pending的signal设置位图
                · kill发送消息时,添加一个sigqueue消息
            · task_struct结构中pending有关于本进程中未决信号的数据成员
            · 实时信号到达时,不管注册没注册,都会重新注册一次,同时再添加一个sigqueue
            · 非实时信号到达时,如果已经有pending信号,就会丢弃当前信号
        · 可靠信号与不可靠信号
            · 信号值小于SIGRTMIN的信号都是不可靠信号
            · 信号值位于SIGRTMIN和SIGRTMAX之间的信号都是可靠信号
        · 实时信号和非实时信号(和可靠不可靠定义类似)
            · 信号值小于SIGRTMIN的信号都是非实时信号
            · 信号值位于SIGRTMIN和SIGRTMAX之间的信号都是实时信号
            · 实时信号支持排队,非实时信号不支持排队
        · 触发
            · 如果信号发送给一个正在睡眠的进程,如果进程睡眠在可被中断的优先级上,则唤醒进程(中断优先级比进程当前执行优先级高,就会执行优先级高的);否则(不可中断)仅设置进程表中信号域相应的位(位图),而不唤醒进程。如果发送给一个处于可运行状态的进程,则只置相应的域即可。
            · 内核处理一个进程收到的信号的时机是在一个进程从内核态返回用户态时。所以,当一个进程在内核态下运行时,软中断信号并不立即起作用,要等到将返回用户态时才处理。进程只有处理完信号才会返回用户态,进程在用户态下不会有未处理完的信号
            1 进程在从系统调用返回之前检测是否收到信号
            2 时间中断,中断执行完之前检测是否收到信号
        · 处理信号有三种类型:
            · 进程接收到信号后退出;
            · 进程忽略该信号;
            · 进程收到信号后执行用户设定用系统调用signal的函数。
        · 信号的安装
            · linux主要有两个函数实现信号的安装:signal()、sigaction()
        · 信号的嵌套
            sa_mask指定在信号处理程序执行过程中,哪些信号应当被阻塞。缺省情况下当前信号本身被阻塞,防止信号的嵌套发送,除非指定SA_NODEFER或者SA_NOMASK标志位。

}   

;{    Cache cpu 操作系统45讲
    LRU 程序局部性:
        Cache存在的原因
    cache由高速静态存储器(cache和内存交换数据的最小单位是一行),地址转换模块,cache行替换模块(脏位 回写位 访问位)组成
    一致性:
        1. 一个 CPU 核心中的指令 Cache 和数据 Cache 的一致性问题
        2. 多个 CPU 核心各自的 2 级 Cache 的一致性问题
        3.CPU 的 3 级 Cache 与设备内存,如 DMA,网卡帧储存,显存之间的一致性问题.这里我们不需要关注这个问题
        Cache数据同步协议:
            MESI MOESI 
            cache硬件会监控数据,并解决数据一致性
}

;{    C++11新特性 C++ primer plus(6) paragraph 18.1
    · 新增long long内置类型 支持64位
    · 支持{}初始化列表,包括内置类型和自定义的类类型
        1. 缩窄
            char chr = {8888}; //编译错误,禁止往更窄类型转换
    · 新的声明
        ...
    · explicit
        class plebe{
            explicit plebe(int num) {}; // convert int to plebe;
            explicit operator int() const {}; // convert plebe to int;
        };
    · 移动语义和右值引用 // 计算机移动文件,文件的存储位置实际不变,只是修改文件的描述信息,这叫移动语义
        class Useless{
            Useless(Useless && us){} // 编写移动语义,第一步,实现移动构造函数
            Useless operator=(Useless && us) {} // 这里是移动赋值函数
        }

        Useless one, three;
        Useless four(one + three); // 这里调用上面的移动构造函数, 编写移动语义,第二步,传入右值,调用移动构造函数
            // 即one+three 不会创建临时对象,再拷贝给four
    paragraph 18.2.5 强制移动
        Useless choices[10];
        Useless best;
        int pick;
        best = choices[pick]; // 拷贝赋值函数
            // static_cast<Useless &&> choices[pick] 静态编译期的强转成右值类型
            // C++11 提供std::move()函数,可以转换成右值类型
        
        实现移动语义
        1. 第一步,提供移动构造或者移动赋值函数
        2. 传入右值
        3. 即使提供了右值,但没有实现移动构造或移动赋值,就会使用复制构造或复制赋值函数
        4. 如果有一个构造函数,但没有提供复制构造函数或复制赋值函数,同样也不会触发移动语义
}

;{    malloc 深入理解计算机CSAPP
    malloc分配内存
        lt 128K 在data segment区域末尾的heap区 + size
            使用brk(void *) sbrk(int) 设置data segment结束位置, 分配可读写内存块
            sbrk(int size)
                size > 0 将program break向栈的方向移动size
                size = 0 返回当前program break的地址(void*)p,但不可读写
                    经由brk((int*)p + 1) 设置data segment结束位置,分配一个int大小可读写的内存块

        gt 128K mmap 在 堆和栈中间再找一块可用空间(TOP_DOWN | DOWN_TOP)

        strace可以看到brk sbrk mmap等函数调用输出日志
    
    alloca(): 动态的在栈上分配空间,并自动释放
}

;{    智能指针 C++ primer plus(6)
    · RAII
    paragraph 16.2 智能指针类模板
        template<class _Ty>
        class auto_ptr
        {	// wrap an object pointer to ensure destruction
            public:
                typedef _Ty element_type;

                explicit auto_ptr(_Ty * _Ptr = nullptr) noexcept : _Myptr(_Ptr)
                {	// construct from object pointer
                }
                ...
                ~auto_ptr() noexcept
                {	// destroy the object
                    delete _Myptr;
                }
                ...
                auto_ptr(auto_ptr& _Right) noexcept : _Myptr(_Right.release())
                {	// construct by assuming pointer from _Right auto_ptr
                }
                ...
                _Ty * release() noexcept
                {	// return wrapped pointer and give up ownership
                    _Ty * _Tmp = _Myptr; // 先把原来的指针作为临时指针返回回去,付给新的quto_ptr
                    _Myptr = nullptr; // 再把原来的指针指向NULL; 后续再访问就会崩溃
                    return (_Tmp);
                }
                ...
            private:
	            _Ty * _Myptr;	// the wrapped object pointer
        }
        auto_ptr<string> auto_str( new string("123") ); // 只能使用explicit 构造
        auto_ptr<string> auto_str = new string("123") ; // 禁止使用implicit 构造
        auto_ptr<string> auto_str(&string("123") ) ; // 禁止构造非堆区内容

        0. 智能指针都有在生命周期结束的时候,自动析构掉指向的堆内容

        1. uniq_ptr 和auto_ptr 重新赋值左值都会夺取原有的控制权
            uniq_ptr会出现编译错误, 但允许临时右值赋值(C++11)
            auto_ptr对原来指针的访问会出现运行时错误,夺走控制权的时候被赋值NULL(被弃用的原因)

        2. shared_ptr指向的时候,会有引用计数的概念,允许同时多个指向

        3. uniq_ptr还多一个数组类型相较其他智能指针

        4. shared_ptr提供一个auto_ptr转型的构造函数

        auto_ptr<string> auto_str1 = auto_str; // 禁止多个智能指针指向同一个对象的地址
                // auto_ptr 会夺走原有的智能指针的控制权,导致原来的智能指针指向NULL,这时候访问该内容会出现崩溃
            // shared_ptr 每增加指向同一个对象的指针,引用计数就加一
            // uniq_ptr 只限定一个指向对象的指针,auto_ptr也有这个特性

        uniq_ptr<string> uniq_str(new (string("123")));
        uniq_ptr<string> uniq_str1 = uniq_str;
            //这里报错不会出现在运行时,编译期间就会报错,禁止多个uniq_ptr指向同一个对象地址

        uniq_ptr<string> demo(char *s)
        {
            uniq_ptr<string> tmp(new string(s));
            return tmp;
        }

        uniq_ptr<string> ps = demo("123"); // 这里从demo返回的uniq_ptr控制权被夺走,无法对临时智能指针访问,编译器允许这种写法
        // 进一步描述, 编译器允许uniq_ptr被一个临时右值赋值,因为临时右值没有机会再访问

        uniq_ptr<string> pt = std::move(ps) // std空间提供uniq_ptr给uniq_ptr赋值

        uniq_ptr<double[]> pt(new double(5)); // 配对使用delete[]; uniq_ptr比auto_ptr多一个数组类型
}

;{    计算机网络 计算机网络:自顶向下 paragraph 3
        3. 运输层TCP
            1. 差错检测(校验和)
            2. ACK+ Sequence 是否冗余 累计确认
            3. 重传(倒计数定时器)
                超时,或者没有收到ACK
                1. 发送端每次发送一个分组,就启动一个定时器
                2. 发送端响应定时器中断
                3. 发送端终止定时器
            4. 协议的收发两端必须缓存多个分组,这些分组是没有ACK确认的,已经确认的可以不用
            5. 回退N步算法(GO BACK N)也被叫做滑动窗口协议
                1. 流水线中未确认的分组数不能超过某个最大允许数N,这被叫做窗口长度
                    这是流量控制对发送方施加限制的一个原因
                2. base为最早的未确认分组的序号
                3. nextseqnum下一个待发分组的序号
                4. 超过N的不可用
                5. 接收方的ACK 是采取累计确认的,0-99 + ACK,100-199 + ACK
                    · 并且一次向上交付一个分组,所以这个分组前的也应该已经交付了
                    · 接收方丢弃所有失序分组,期望接收分组N,却收到了N+1,丢弃N+1,就可以重发
                6. 分组多的情况下,连接里,重发的分组会很多
            6. 选择重传(Selective Repeat)
                1. 接收方确认一个正确接收的分组而不管其是否按序
                2. 接收方失序的分组将被缓存直到所有丢失分组(序号更小的分组)都被收到为止.这时就可以将一批分组按序交给上层
            7. MSS MTU
                · TCP可从缓存中取出并放入报文段中的数据数量受限于MSS(Maximum Segment Size 最大报文段长度 1500 - TCP/IP首部长度40)
                · 一般根据本地发送主机发送的最大链路层帧长度MTU(Maximum Transmission Unit 最大传输单元 1500),除掉TCP/IP首部长度40
                · 当TCP发送一个大文件时,比如一个图像,TCP通常将该文件划分成长度为MSS的若干块;telnet这样的应用交互传送的数据一般比较小,也许只有21个字节
            8. TCP连接的组成:
                · 传输层的发送缓存,接收缓存,套接字
                · TCP连接收到正确,按序的字节后,就把数据放入到接收缓存中
                · 应用层读取的速度小于传输层发送的速度,就会出现接收缓存溢出,所以需要流量控制服务(Flow Control Service)
            9. TCP报文头字段
                · 序号: 按照要发送的字节流编号
                    50000个字节,MSS=1000,TCP传输层将为该数据流分成50个报文段,第一个报文段序号0,第二个1000 ···
            10. 快速重传
                · 发送方接收到对相同数据的3个冗余ACK,TCP就会执行快速重传之前丢失的报文段
            11. TCP是GBN和SR的结合体
            12. 流量控制服务(Flow control Service) - 与报文中接收窗口字段配合
                · 发送方维护接收窗口(receive window)的变量rwnd
                · 接收缓存大小为RecvBuffer,应用层不时的从接收缓存中读取数据
                    LastByteRead:应用层读取的数据流的最后一个字节编号
                    LastByteRcvd:网络传输的数据放入接收缓存的数据流的最后一个字节的编号
                · LastByteRcvd - LastByteRead <= RecvBuffer才不会溢出
                · 那么发送方收到的rwnd = RecvBuffer - (LastByteRcvd - LastByteRead)
                    LastByteSent:最后发送的字节流的一个字节编号
                    LastByteAcked:最后发送的字节流的被确认的一个字节编号
                    然后发送方的 LastByteSent - LastByteAcked <= rwnd 保证发送的数据不会撑爆接收缓存
                · rwnd=0时,发送方会继续给接收方发送只有一个字节的报文段
                · UDP没有流量控制,会将缓存溢出
            13. 拥塞控制(Congestion Control)
                · TCP发送方也可能因为IP网络的拥塞而被遏制
                    路由器由于网络拥挤导致接收缓存溢出,导致这部分报文丢失
                    报文丢失(一段时间没有收到ACK),就会产生分组重传
                · 分组重传可以作为网络拥塞的征兆
                1. 端到端拥塞控制
                    · TCP分配的缓存和变量,在前面rwnd(接收窗口)等变量的基础上,再加上一个额外的变量cwnd(Congestion Window),小于两个窗口中较小的一个
                        LastByteSent - LastByteAcked <= min {rwnd,cwnd}
                    1. 拥塞检测
                        超时或者三次冗余确认导致的重传被认为是网络拥塞的一个迹象,TCP会相应的减小窗口长度N
                    2. 发送方收到ACK后,认为网络不拥塞,可以增加发送方的速率
                    3. TCP拥塞控制算法
                        0. ssthresh值
                            cwnd>=ssthresh时,cwnd开始谨慎+1
                        1. 慢启动
                            · CWND初始速率为MSS/RTT,在收到ACK确认报文后,CWND设置为2,速率变为2MSS/RTT,一次发两个报文段;同时收到这两个的ACK之后,CWD=2,再同时发送4个出去;收到4个ACK后,CWND+=4;以此类推,处于一个慢启动的过程
                            · 结束增长的时机
                                1. 检测到拥塞,CWND重新设置为1,重新开始慢启动,并将ssthresh指设置为cwnd/2
                                2. cwnd>=ssthresh时,结束慢启动,进入拥塞避免模式
                                3. 检测到3个冗余ACK,进入迅速恢复状态
                            ssthresh为1
                        2. 拥塞避免
                            TCP更为谨慎的增加CWND
                            1. CWND值减为上次遇到拥塞时的一半
                            2. CWND的值每次只增加一个MSS
                            3. ssthresh变为一半
                            4. 进入快速恢复状态
                        3. 快速恢复
                            1. 对收到的每个冗余ACK,cwnd都增加一个MSS
                            2. 当收到ACK确认报文时,才降低cwnd进入拥塞避免
                            3. 当超时时,会进入慢启动状态
                            4. 当丢包事件出现时,cwnd=1MSS,ssthresh=cwnd/2
                        4. TCP Tahoe算法
                            不管是发生超时时指示的丢包事件,还是发生3个冗余ACK指示的丢包事件,都无条件的将cwnd=1MSS,并进入慢启动
                        5. TCP Reno算法
                            在丢包时,cwnd每次只加+1,尽管已经进入慢启动状态
                        6. TCP拥塞加性增,乘性减
                            cnwd开始每次加1,遇到3ACK丢包事件,cwnd减半
                        7. 目前lunux采用CUBIC算法
                2. 网络辅助的拥塞控制
                    交换机间维护这一条VC虚电路,逐个VC的状态允许交换机跟踪各个发送方的行为
            14. 三次握手
                · 服务器在收到客户端的ACK之后,为该TCP连接分配TCP缓存和变量(这里分配的内存会因为SYN攻击暴增)
                · 客户端收到服务端的SYNACK报文后,为该TCP连接分配TCP缓存和变量,另外发给服务端的SYNACK报文里的SYN为0,因为连接建立完成了,以后都是0.这时的报文段里可以负载数据了
                · 三次握手中服务端发送SYNACK之后,一段时间后没收到SYNACK就会断开这个SYN_RCVD半连接状态,并释放内存
            15. 四次挥手
                · TCP连接双方都可以关闭连接,连接结束后,释放缓存和变量分配的内存
            16. TCP状态机(Finite Status Machine)
                ·客户端CLOSED状态------客户发起连接发送SYN报文--->客户端SYN_SENT状态--收到SYNACK后发送SYN(0)+ACK-->客户端ESTABLISHED状态
                ·客户端准备关闭连接,发送FIN报文,进入FIN_WAIT_1状态----收到服务端的ACK--->客户端FIN_WAIT_2状态----收到服务端FIN(1)---->客户端TIME_WAIT状态----客户端发送ACK---->客户端CLOSED状态
                    如果TIME_WAIT状态下发出去的ACK丢失了,服务端会重传FIN(1),然后重发ACK,如果发出去了,服务端就会关闭;如果没有发出去,在TIME_WAIT这段时间里,客户端会不停重试,直到时间过后,最终都会关闭
                · 服务端开始时CLOSED状态----主动创建socket--->服务端LISTEN状态-----收到SYN报文,并发送SYNACK--->服务端SYN_RCVD状态---收到SYNACK--->服务端ESTABLISHED----收到FIN,发送ACK---->服务端CLOSE_WAIT状态----服务端发送FIN---->服务端LAST_ACK状态----收到ACK---->服务端CLOSED状态
            17. SYN cookie
                服务端收到SYN报文连接请求后,不急着创建半开连接分配内存,而是先缓存下来,直到下次发来的ACK是经过特殊函数生成的sequence+1的才创建全开连接
            18. 丢包
                · 连续发送三次冗余ACK
                · 超时

        4. 网络层
            0. 网络层组成
                1. 路由选择协议,进行路径选择,负责生成转发表
                2. IP协议,IP数据报字段
                3. ICMP协议,报告数据包中的差错和对某些网络层信息请求进行响应
            1. 构造网络层分组交付的方法
                1. 数据报模式
                2. 虚电路模式
            2. 转发(forwarding)
                · 当一个分组到达路由器的一条输人链路时,路由器必须将该分组移动到适当的输出链路
                · 每台路由器都有一张转发表
            3. 路由选择(routing)
                当分组从发送方流向接收方时,网络层必须决定这些分组所采用的路由或路径.计算这些路径的算法被称为路由选择算法(routing algorithm)
            4. 网络层提供了单一的服务,称为尽力而为服务,各种得不到保证
            5. 网络层的连接和无连接
                1. 有连接的称为虚电路(virtual Circuit)网络
                    · 路由器必须为进行中的连接维持连接状态信息
                        · 创建一条新的虚电路,转发表就新增一个新表项
                        · 终止一条新的虚电路,转发表就删除对应的表项
                    · 虚电路三个阶段
                        1. 虚电路建立
                        2. 数据传送
                        3. 虚电路拆除
                    · 端系统向网络发送指示虚电路启动和终止的报文,以及路由器之间传递的用于建立虚电路的报文,统称为信令报文,交换报文的协议称为信令协议
                2. 无连接的称为数据报网络
                    1. 网络层发送分组,在分组前加一个目的IP
                    2. 分组传递过程中的路由器使用目的IP前缀做转发,前提是在路由器中有一个将目的IP前缀映射到链路接口的转发表,映射时拿目的IP前缀匹配
                    3. 当分组到达路由器时,路由器就根据目的IP前缀转发到对应的接口去
                    4. 目的IP前缀匹配采用最长前缀匹配规则,避免一个目的IP指向多个接口
                    5. 路由器中的转发表通过路由选择算法进行修改,每隔1-5分钟左右更新一次
            6. IP层数据报格式
                · TTL Time To Live
                    数据报每经过一台路由器时,该字段值就-1,为0,就会被丢弃
                · 标识,标志,片偏移
                    与IP分片有关
                · 协议
                    指示了IP数据报的数据报部分应该交给那个运输层协议,6代表TCP,17代表UDP,这将传输层和运输层绑在一起
                · 首部校验和
                    将IP首部的数据每两个字节当作一个数,最后算出所有和的反码
                · 数据(有效载荷)
                    这里包含运输层报文段或者ICMP报文
            7. IP数据报分片
                · 分片(fragment)(IPV6已经废止了分片)
                    不同路由器设备间的MTU协议不一样,1500的MTU过大就要分成多个较小的IP数据报交给数据链路层,然后发送出去
                    1 标识
                        一个完整的数据报分片后,每个分片都一个标识号,表示这几个分片构成一个数据报
                    2 标志
                        为了确定收到了最后的一个片,用标志0标识,其他的为1
                    3 偏移字段
                    每个片在完整数据报上的偏移
                · 组装
                    · 片在到达端(PC)系统的运输层前需要重新组装
                    · 为了组装这些片,需要将标识,标志和片偏移字段放在IP数据报首部中
                        整个完整的分片组装完成后才会交给运输层,如果有一个或多个片没有到达,则不完整的数据报会被丢弃(TCP会控制重传)
            8 IPv4编址
                1. 无类别域间路由选(Classless Interdomain Routing, CIDR)
                    · 格式
                        a.b.c.d/x a.b.c.d前X位称为该地址的前缀,一个子网共享同一个前缀,这相当大的减少了在路由器中转发表的长度;32-x比特用来区分该子网的内部设备的
                2. 分类编址(classful addressing)
                    · 8/16/24分别称为A/B/C类网络
                    · 广播地址255.255.255.255用来给子网内所有主机发送报文
                3. 动态主机配置协议(Dynamic Host Configuration, DHCP)
                    · DHCP是一个4个步骤的过程(UDP)
                        1. DHCP发现
                            客户给255.255.255.255发送广播DHCP发现报文,让新到的主机发现DHCP服务端
                        2. DHCP提供
                            DHCP服务器收到DHCP发现报文后向客户做出响应,仍然使用255.255.255.255广播,子网中可能有多个DHCP服务器
                        3. DHCP请求
                            新到的客户端从中选择一个服务器,然后发送一个DHCP请求报文
                        4. DHCP ACK
                            服务器对DHCP请求做出确认
                4. 网络地址转换(Network Address Translation, NAT)
                    · NAT转换表,从外网侧进入路由器转发到子网各个端口
                        外网IP+端口 -> 内网IP+端口
                    · NAT穿越
                        A->B->C A和C称为穿越
            9. 因特网控制报文协议(ICMP)
                · ICMP是被主机和路由器用来彼此沟通网络层的信息,最典型的用途是差错报告
                · 差错报告
                    · '网络不可达'的错误报文
                · Traceroute
                    为了判断源和目的地之间所有路由器的名字和地址,源主机向目的主机发送一系列普通的IP数据报,这些数据报都携带了具有一个不可达UDP端口号的UDP报文段(使用TTL来控制,第一个TTL=1,第二个为2...);当这个报文到达第N个路由器,路由器发现TTL=0,就会丢弃该数据报并给源主机发送一个ICMP告警报文(类型11编码0,TTL过期),这个告警报文包含路由器的名字和IP,并且当ICMP到达源主机时,源主机根据定时器得到往返时延,得到第N个路由器的名字和IP地址
            10. 路由选择算法(最小路径选择)
                1. 全局式路由选择算法,常被称为链路状态算法(Link Status,LS)
                    · OSPF路由选择协议中使用链路状态广播算法(Link State Broadcast)获得网络中所有其他节点广播链路状态的特征和费用
                        1 Dijkstra算法
                            · 定义三个接口
                                list LiNode // 存储所有未遍历的节点,不包括起点
                                Dis(start, node) // 起点到node节点的最短路径
                                PreNode() // 获取当前最短路径的前一个结点
                                Cost[node1, node2] // 记录相邻节点间的cost
                                1. 初始化一个获取Dis(start, node)的列表
                                    table[start, node] = INFINITE
                                    //和start是临接点的重新初始化
                                2. 遍历总共N个节点
                                    while(N--){
                                        node = getNodeCostMin(Dis(start, Node(N)));//每次选择一个Dis(Node)最小的加入进来
                                        Node.pop(node);

                                        for (tmp : LiNode) //遍历剩下邻接node
                                        {
                                            Dis(start, tmp) = min(Dis(start, tmp), Dis(start, node) + Cost(node, tmp)); // 再新增一个遍历节点时,从起点到邻接node的Dis要么是已有的Dis(start, tmp),要么是Dis(start,node)+从当前点到邻接点新增的一段cost
                                        }

                                    }
                        2 距离向量路由选择算法(Distance-Vector, DV)
                            1. 分布式
                                每个节点都和周围节点交互接收信息,再将计算结果分发给邻居
                            2. 迭代
                                这个过程要持续到邻居之间无更多信息要交换为止(一般会自动停止)
                            3. 异步
                                各个节点之间独立操作
            11. 因特网中的路由选择
                1. RIP(Routing Information Protocol) 因特网中自治系统内部的路由选择
                    · RIP响应报文(RIP response msg) 又称RIP通告(RIP advertisement)
                        · 邻接路由器或主机每隔30秒交换一次报文,报文中包含了一个该AS内的多达25个目的子网的列表,以及发送方到其中每个子网的距离
                        · 路由器超过180秒没收到邻居的RIP通告报文,这个邻居就是不可达的.并且修改本地路由选择表,向周围邻居传播该消息
                        · 路由器使用UDP再520端口上发送RIP报文
                        · RIP是一个运行在UDP上的应用层进程
                    · 路由选择表(Routing table)
                        包括该路由器的距离向量和转发表(目的子网,下一台路由器,到目的地的跳数)
                    · 这个使用DV算法
                2. OSPF(Open Shortest Path First) 开放最短路优先
                    · RIP一般放在下层ISP中,OSPF在上层ISP中
                    · OSPF,向自治系统内所有路由器广播路由选择信息,RIP只向邻居广播
                    · 每隔30分钟或者一条链路的状态发生变化
                    · OSPF运行在IP层
                    · 这个使用LS中的Dijkstra算法
                3. 自治系统间的路由选择 BGP(Boder Gateway Protocol)
            12. 广播(broadcast routing)和多播路由(multicast routing)选择
                1. 广播路由选择算法
                    1. 无控制洪泛
                        · 每次经过一个节点就对子网进行复制报文并分发,当网络拓扑存在环的情况下,会导致无休止的循环,导致广播风暴
                    2. 受控洪泛
                        1. 序号控制洪泛
                            源节点将其地址和广播序号放入广播分组,再向邻居发送分组.每个节点维护已经收到的,复制的和转发的源地址和每个广播分组的序号列表.如果收到的分组的序号已经在列表中,就丢弃
                        2. 反向路径转发(Reverse Path Forwarding, RPF)
                            来的时候,分组是根据最小路径发来的,所以返回的时候,也是发来的路径. 如果是别的路径发来的就会被丢弃
                        3. 生成树广播
                            确定一个中心点作为广播中心,生成一个最小生成树
                        4. 实践中的广播算法
                            1. 应用层在TCP中添加序号
                            2. OSPF应用序号控制洪泛
            13. 多播
                · IGMP通知一个路由器:主机运行的一个应用程序想加入一个特定的多播组
        5. 链路层
            5.1.1 链路层提供的服务
                1. 成帧
                2. 链路接入
                    媒体访问控制(Medium Access Control, MAC)协议规定了帧在链路上的传输规则,点对点链路和多路访问
                3. 可靠交付
                4. 差错检测和纠正
            5.1.2 链路层在何处实现
                · 链路层的主体部分是在网络适配器(Network Adapter,也称网络接口卡Network Interface Card, NIC)中实现的.
                · 适配器核心是链路层控制器,里面提供链路层服务
                · CPU中的部分链路层软件提供组装链路层寻址信息和激活控制器硬件
                · 接收端,链路层软件响应控制器中断,开始处理差错条件或者向上传递数据报
            5.2 差错检测和纠正技术
                5.2.1 奇偶校验
                    在发送的d个比特数据之后设置一个奇偶位,使得这d+1个比特中出现1的个数是奇数或者偶数
                5.2.2 校验和方法
                    · 运输层采用校验和,链路层使用CRC
                5.2.3 循环冗余检测(Cyclie Redundancy Check, CRC),也称为多项式编码
                    · d比特数据+r比特CRC多项式,使用d+r比特数据除以多项式最后余数为0
                    · 异或操作: 没有进位的减法
                    · D*2^r XOR R = nG
                        R = remainder(D*2^r/G)
            5.3 多路访问链路和协议
                · 碰撞
                    传输的帧在所有的接收处碰撞(collide),碰撞发生,没有一个接收节点能有效的获得任何传输的帧(碰撞的帧的信号纠缠在一起)
                5.3.1 信道划分协议
                    · 时分多路复用(TDM)
                    · 频分多路复用(FDM)
                    · 码分多址(Code Division Multiple Access, CDMA)
                5.3.2 随机接入协议
                5.3.3 轮流协议
            5.4 交换局域网
                5.4.1 链路层寻址和ARP
                    1. MAC地址 又称 LAN地址 物理地址
                        6个字节大小
                    2. 地址解析协议(Address Resolution Protocol)
                        ·主机或路由器在内存中具有一个ARP表,包含IP到MAC的映射
                        · 发起方用MAC广播地址(FF-FF-FF-FF-FF-FF)发送ARP查询分组,然后收到IP和MAC映射的ARP分组,更新本地的ARP表
                        · ARP是即插即用的,如果主机和子网断开,这个主机的映射就会被删掉
                5.4.2 以太网
                    1. 以太网帧
                        · 数据字段
                            以太网的最大传输单元(MTU)是1500,超过1500,就会分片
                        · 类型字段
                            区别IP/ARP协议内容
                        · 前同步码
                            同步两个网卡的发送和接收
                5.4.3 链路层交换机
                    1. 交换机转发和过滤
                        · 交换机表,包含MAC地址,网卡接口,过期时间
                    2. 交换机和路由器的区别
                        · 交换机会有广播风暴的问题,路由器有广播多播算法
                        · 2/3层
                5.4.4 虚拟局域网
                    · 交换机中维护一个端口到vlan的映射表

}

;{    锁有哪些 操作系统45讲
        互斥锁 自旋锁 读写锁 (统称悲观锁)
        CAS锁 (乐观锁)
        分布式锁
        1. 互斥锁 mutex
            互斥锁加锁失败会释放CPU,调度其他线程;自旋锁加锁失败会一直占用CPU直到加锁成功
        2. 无锁CAS 无锁队列
            static inline unsigned __sl_cas(volatile unsigned *p, unsigned old, unsigned new)
            {
                __asm__ __volatile__("cas.l %1,%0,@r0"
                    : "+r"(new)
                    : "r"(old), "z"(p)
                    : "t", "memory" ); // *p == old ? new = old : new = new;
                return new;
            }

            // 使用CAS锁实现自旋锁
            static inline void arch_spin_lock(arch_spinlock_t *lock)
            {
                // 1 -> 0 自旋锁加锁成功
                while (!__sl_cas(&lock->lock, 1, 0));
            }

        3. 自旋锁 协调多核心 CPU
            ...
            关闭中断,避开中断影响
            lock前缀 锁住当前CPU和内存的总线通信,防止多个CPU同时访问内存中的变量
            自旋锁没有获得资源的时候,会不停占用CPU,对于很多不能马上获取资源的情况下,会很浪费CPU,所以改良成信号量
        4. linux读写锁
            1. 读写锁也称共享-独占锁 Shared-Exclusive
                读取时:加的共享锁,写入时:独占锁(互斥)
                读写锁本质是自旋锁的变种
            2. 读写锁非常适合读取数据的频率远大于修改数据的频率的场景
            3. 已经有读锁,可以再加读锁;不能再加写锁,读写互斥
            4. 已经有写锁,不能再加读锁,读写互斥;也不能再加写锁,写写互斥
            5. read_lock,write_lock RW_LOCK_BIAS=0x1000000
                read_lock: lock - 1
                write_lock: lock - 0x1000000
                加锁失败的时候,进入自旋锁的步骤,直到加锁成功
        5. 悲观锁 (认为线程间同时修改共享资源的概率比较高)
            互斥锁,自旋锁,读写锁,都是悲观锁
            // 读写数据的时候,预先加锁排他
        6. 乐观锁 (认为线程间同时修改共享资源的概率比较低)
            CAS 
            // 预先不加锁,通过原子指令排除并发

        7. 原子操作(只适合单体变量)
            a++改成原子++,鉴于不同芯片对原子++操作的支持,这里写一手汇编代码
            汇编嵌入:
                volatile的语义
                    1. 强制对修饰的变量,读取时从内存中读取,写入时,立即写入到内存中. 避免使用不同cpu的cache里存储的值,而不是内存里的值
                    2. 禁止指令重新排序
                typedef struct {
                    volatile s32_t a_count; // 访问变量时,强制从内存读写
                } atomic_t;

                static inline void atomic_add(int i, atmoic_t *v)
                {
                    // lock 锁住CPU到内存的总线,只有当前CPU核心可以访问该变量,实现原子操作
                    // volatile 禁止指令重新排序
                    __asm__ __volatile__("lock;" "addl %1,%0":"+m" (v->a):"ir" (i));
                }

        8. 开关中断和单线程同步
            void hal_cli(){
                // cli 关闭中断 clear interrupt
                __asm__ __volatile__("cli":"memory");
            }

            void hal_sti(){
                // sti 开启中断 set interrupt
                __asm__ __volatile__("sti":"memory");
            }

            void foo()
            {
                hal_cli();
                ...
                hal_sti();
            }
            嵌套版改善:每次关中断之前先压入一次eflags,开中断都是恢复这个eflags,如果有多层,根据栈的特性关几次,就要开几次才能开中断

        9. 信号量 CPU时间管理大师 // 进程互斥
            1. 用自旋锁包装信号量
                信号量的加锁/解锁,分别用自旋锁控制进入
            2. 对信号量资源不够的情况下进入睡眠队列,等待唤醒,避免占用CPU
            3. schedule_timeout 让当前调用进程进入睡眠
                wake_up_process 从信号量的休眠队列中唤醒进程
        10. 分布式锁
            · 同步不同机器的进程的锁
            · 涉及范围
                数据库(redis,mysql),zk
                · redis锁
                    · 进程挂掉导致死锁,超时解锁
                    · 四元组
                        srcip:srcport:starttime(保证顺序):pid
                    · setnx加锁
                        setnx("lock", 四元组, ..., expiretime)
                        · 新建一个key
                        · 超时时间到了, 执行未完成可以续时间
                    · 释放锁+事务(watch命令)
                        if(get(key) == 四元组)
                            delete(key)
                    

}

;{    内存泄漏/野指针 Linux-Unix系统编程手册
    ·glibc: malloc/free/mtrace/muntrace/mcheck/mprobe/MALLOC_CHECK_ 环境变量 //以上都可以参考glibc手册
        另外有一些非标准库函数mallopt()/mallinfo()
    ·malloc调试,glibc提供了和标准malloc函数包相同的API,但附加了捕获内存分配错误的功能。要使用调试库,需要在编译时链接,这些库有Electric Fence、dmalloc、Valgrind、Insure++
}

;{    中断 软件调试
    · 中断来自外部设备,异常(把由INT n指令称为软件中断)是cpu主动产生的
    1. cpu触发(hal层,硬件层)
        cpu->中断门描述符->异常处理入口     ->异常分发器    ->中断异常描述符
                        ->中断处理入口     ->中断分发器    ^
    2. 中断控制器
        多个设备的中断信号线都会连接到中断控制器上,中断控制器可以决定启用或者屏蔽哪些设备的中断,还可以决定设备中断间的优先级
    3. 异常
        故障:页错误
        陷阱:系统调用
        中止:不可恢复的错误,转到内核的abort(),结束应用进程
}

;{    wine
    lightdm-kwin-xorg
    0. wine-server
        读取数据,读取一个request类型, read(fd, &val, sizeof(type)) != sizeof(type),  就要循环读;
    1. wine -> preloader ->wine
        client:
            fd_socket = server_connect();
                s = socket( AF_UNIX, SOCK_STREAM, 0 )) 
                connect( s, (struct sockaddr *)&addr, slen ) != -1)
                return s;
            ntdll_get_thread_data()->request_fd = receive_fd( &version );
                recvmsg( fd_socket, &msghdr, MSG_CMSG_CLOEXEC )) > 0) // client收到server-thread发送来的cmsg_contorl = fd: request_pipe[1]; msg_iov = SERVER_PROTOCOL_VERSION
                    msghdr.msg_iov     = &vec;
                    vec.iov_base = (void *)handle;
                    vec.iov_len  = sizeof(*handle);
                    for (cmsg = CMSG_FIRSTHDR( &msghdr ); cmsg; cmsg = CMSG_NXTHDR( &msghdr, cmsg ))
                    {
                        if (cmsg->cmsg_level != SOL_SOCKET) continue;
                        if (cmsg->cmsg_type == SCM_RIGHTS) fd = *(int *)CMSG_DATA(cmsg);
                    }
                    return fd;
            (version != SERVER_PROTOCOL_VERSION) // server-client比较版本
            ...
            init_thread_pipe
                server_pipe( reply_pipe )
                wine_server_send_fd( reply_pipe[1] );
                    cmsg->cmsg_level = SOL_SOCKET;
                    cmsg->cmsg_type  = SCM_RIGHTS;
                    *(int *)CMSG_DATA(cmsg) = fd;
                    msghdr.msg_controllen = cmsg->cmsg_len;
                    vec.iov_base = (void *)&data;
                    vec.iov_len  = sizeof(data);

                    data.tid = GetCurrentThreadId();
                    data.fd  = fd;
                    if ((ret = sendmsg( fd_socket, &msghdr, 0 )) // 客户端给server发送写pipe端,和自己的线程id


                wine_server_send_fd( ntdll_get_thread_data()->wait_fd[1] );
                ntdll_get_thread_data()->reply_fd = reply_pipe[0]; // 客户端自己留一个读pipe端
            ...
            wine_server_call( req );
                if ((ret = send_request( req ))) return ret;
                     SERVER_START_REQ( init_first_thread )
                        write( ntdll_get_thread_data()->request_fd, &req->u.req, // 给线程的request_pipe[1]发送req


        启动wine-server, 
        wine向服务端发送"sock"域内套接字,服务端不主动消息,但是发回连接套接字
        listen
            client = accept( get_unix_fd( master_socket->fd )
            process = create_process( client,
            create_thread( -1, process, NULL );
                (pipe( request_pipe )
                close( request_pipe[1] );
                
                send_client_fd( process, request_pipe[1], SERVER_PROTOCOL_VERSION ) 
                    send_client_fd( struct process *process, int fd, obj_handle_t handle )
                        cmsg = CMSG_FIRSTHDR( &msghdr );
                        *(int *)CMSG_DATA(cmsg) = fd;
                        msghdr.msg_controllen = cmsg->cmsg_len;

                        msghdr.msg_iov     = &vec;
                        msghdr.msg_iovlen  = 1;

                        vec.iov_base = (void *)&handle;
                        vec.iov_len  = sizeof(handle);
                        ret = sendmsg( get_unix_fd( process->msg_fd ), &msghdr, 0 ); // thread给client;process->msg_fd发送一个cmsg_contorl = fd; msg_iov = SERVER_PROTOCOL_VERSION, 这个发送给客户端
                        ...
                        fd = request_pipe[0];
                        if (!(thread->request_fd = create_anonymous_fd( &thread_fd_ops, fd, &thread->obj, 0 ))) // 线程成了读取端
                        set_fd_events( thread->request_fd, POLLIN );
            
            thread_fd_ops:
                thread_poll_event( struct fd *fd, int event )
                    else if (event & POLLIN) read_request( thread );
                        call_req_handler( thread );
                    else if (event & POLLOUT) write_reply( thread );

        process
            client = accept( get_unix_fd( master_socket->fd )
                process = create_process( client,
                    process->msg_fd = create_anonymous_fd( &process_fd_ops, fd
                        ...
                        set_fd_events( process->msg_fd, POLLIN );

            process_fd_ops:
                else if (event & POLLIN) receive_fd( process ); 
                    recvmsg( get_unix_fd( process->msg_fd ), &msghdr, 0 );
                        if (cmsg->cmsg_type == SCM_RIGHTS) fd = *(int *)CMSG_DATA(cmsg); // 收到线程的request_pipe[1]写端
}

;{    位图 bitset
        · 基础数据结构, 选用最长的整形unsgned long long/long, 分配静态数组array[N];
        · set接口
            ((_Array[_Pos / _Bitsperword/*8bit * sizeof(unsigned long long)*/] // pos/(8*8) 确定在哪个base,pos%(8*8)确定在这个long的第几位
			    & ((_Ty)1 << _Pos % _Bitsperword)) != 0);
        · 布隆过滤器
            · biset空间过大的情况下,key:0/1的空间利用率不够
                采用多个hash_func, 充分利用bitset
            · 使用多个hash_func在位图上标记出来
                查找一个key是否存在, 需要对每个hash_func做计算,每个bit都为1,才存在;如果有一个不存在,就肯定不存在
            · 一般这个hash_func会尽量避免冲突
                · 如果冲突,就会在某次查找的时候存在误差,本来不存在的key编程存在的,但这是可以接受的
            · 场景
                ·redis缓存穿透
                    · redis缓存的数据,出现大量数据没有被访问到的情况下直接查询数据库
                    · 加上布隆过滤器,可以避免redis查询的开销;误差的情况可以避免
}

;{    Git
    · merge & rebase
        merge
            把两条独立的分支的修改合并进一条分支, 要想条分支都拥有全量代码,需要互相merge一次
        rebase
            1. 有2条分支, 取出最近的一系列修改, 复制一份留在原地
            2. rebase [branch]合并的时候, 把当前分支拷贝一份,再合并到另一个分支后
            3. 现在表象就是在一条分支上的顺序开发, merge会有两条
    · HEAD
        · HEAD是独立的指针,指向某个提交记录 // 使用git checkout [log]
            branch也是一个指针指向某个提交记录 // 使用git branch -f [branch] [log]
        · cat .git/HEAD 查看当前HEAD指向
        · git symbolic-ref HEAD 对HEAD指向的引用进行解引用
        · 分离的HEAD
            · 正常的HEAD指向是HEAD指向分支,然后指向提交记录
                HEAD->main->log
                分离的HEAD指向提交记录,没指向分支
                    HEAD->log // git checkout commit, 就和分支分离了
    · 相对引用
        · git checkout log太长了不方便
        · <分支名字>^^^^ 从当前引用向前移动n个^提交记录
            此时指针指向HEAD - 4
        · <分支名字>~[num] 从当前引用向前移动num个提交记录
            此时指针指向HEAD~[num]
        · 强制修改分支位置
            git branch -f main [HEAD~4|log]; //移动分支到指定[main]分支的[HEAD~4|log]位置 




}

;{    通信协议MQTT
    · mosquito 消息队列
        读取数据时根据MQTT协议解包
        · mosq->in_packet
            read(sock_fd, &mosq->in_packet);
        · mosq->msg_in
            //QOS=2 发消息的时候,把消息压入msg_in队列
                · inflight数据优先,queue数据压后,
                · handle__publish()
                    · db__message_insert()
                        · db__ready_for_flight()
                            DL_APPEND(mosq->msgs_in.inflight, message);
                        · db__ready_for_queue()
                            DL_APPEND(mosq->msgs_in->queued, msg);
        · mosq->msg_out
                · subs__send()
                    · db__message_insert() 
                        · db__ready_for_flight()
                            DL_APPEND(mosq->msgs_out.inflight, message);
                        · db__ready_for_queue()
                            DL_APPEND(mosq->msgs_out->queued, msg);
            · main_loop()
                · db__message_write()
                    · db__message_remove() //QOS=2 收到PUBCOMP时,删去消息
                        DL_DELETE(msgs_in.inflight, item);
                        DL_DELETE(msgs_out.inflight, item);
                        write(msgs_out.inflight);
                    · db__message_dequeue_first() //queue作为第二层缓冲,数据从queue转化到inflight
                        DL_DELETE(msgs_in->queued, msg);
                        DL_APPEND(msgs_in->inflight, msg);






}

;{    ubusd RPC
    1. RPC invoke
        client_method-----register_method------>ubusd-----avl.insert(method)------
        client_invoke-----inovke(method)---->ubusd----avl_find(method)--->client_method
    2. subcriber-notify 对特定topic的发送
        client_register-----reg_subscriber(topic)-----ubusd---avl.insert----
        client_subscriber----subscribe(topic)----->ubusd-----list_add(subscriber)
        client_notify-----notify(topic)--->ubusd------>find_id(object)---->client_subscriber
    3. event广播 想听广播消息的会收到
        client_reg---reg(pattern)--->ubusd--avl.insert(pattern)---
        client_event---send(pattern)--->ubusd---->avl.traverse(pattern)---send(client_reg)
    4. ubusd
        · 单个连接上来后,EPOLL_IN|EPOLL_OUT都走client_cb,有消息没写完,会进入pending队列,等到EPOLL_IN击中接着写;当前pending队列清空后,EPOLL_CTR(EPOLL_IN)只接受读事件,等到有消息没写完就压入pending队列,EPOLL_CTR(EPOLL_IN|EPOLL_OUT)再打开读写事件
        · 这里没读完的数据 是因为recvmsg一次性读到了需要的数据,blob_buf头里记录了大小,先读到这个大小,再接着读后面的buffer
        · client_fd = accept(UNIX_FD);
            · cl->sock.fd = client_fd; cl->sock.cb = client_cb;
                · client_cb()
                    · ubus_msg_head()
                        · cl->tx_queue[cl->txq_cur];
                    · ubus_msg_writev()
                    · ubus_msg_dequeue();
                    · bytes = recvmsg(sock->fd, &msghdr, 0);
                        · struct msghdr msghdr = {
                                .msg_iov = &iov,
                                .msg_iovlen = 1,
                            };
                            int offset = cl->pending_msg_offset;
                            iov.iov_base = &cl->hdrbuf + offset;
		                    iov.iov_len = sizeof(cl->hdrbuf) - offset;
                        · cl->pending_msg_offset += bytes;
                        · cl->pending_msg_fd = fd_buf.fd;
                        · memcpy(&cl->pending_msg->hdr, &cl->hdrbuf.hdr, sizeof(cl->hdrbuf.hdr));
		                    memcpy(cl->pending_msg->data, &cl->hdrbuf.data, sizeof(cl->hdrbuf.data));
                    · ub = cl->pending_msg;
                        · ubusd_proto_receive_message()
                            · ubus_msg_send(cl, retmsg, false);
                                · written = ubus_msg_writev(cl->sock.fd, ub, 0);
                                    · ubus_msg_enqueue(cl, ub);
                                        // 收到对方的请求,解析INVOKE啥的,转发调用,发送给服务提供者
                                        //如果没有写完就加入队列,下次接着写


            · avl.insert(&clients, id);


        
}

;{    文件
    · inode
        iNode对应文件,文件被写入到磁盘持久化,占用磁盘空间
    · dentry
        由内核维护的一个树状数据结构
        

}

;{    mysql
    1 mysql驱动
        · mysql驱动和数据库建立网络连接,通过驱动发送请求给数据库
            · 每次建立一个socket连接,执行SQL语句,再关闭;这种创建销毁比较浪费,可以考虑重用连接
            · 创建一个数据库连接池(DBCP,C3P0,DRUID) //类似线程池,创建销毁造成费时
                不同的线程根据需要来取
            · 数据库作为服务器收到SQL请求,执行操作
    2. 数据库服务器
        · 查询解析器
            解析sql语句
        · 查询优化器
            查询路径树,找出最优的查找
        · 执行器
            调用存储引擎的接口执行SQL语句
        · 存储引擎InnoDB、MyISAM、Memory
            目前使用InnoDB,执行sql语句的地方
    3. InnoDB
        1 缓冲池
            · 类似pagecache,缓存磁盘的数据到内存中,
                · 如果读取到了,就直接返回结果;否则就要继续读磁盘;
                · 写会有独占锁,写缓存,标记脏页,等待落盘
        2 undo日志
            · 在更新之前,会先写入到undo日志中,等待回滚
            · 提交事务之前,可以回滚到之前的数据
        3 写数据
            1. 先加锁,原来的数据写入undo,写缓存,标记脏页,写数据库
        4 redo日志 buffer
            · 避免宕机,内存数据丢失
            · 每次内存中写入的数据都会生成一个redo buffer
            · 提交事务的时候将redo写入到磁盘中
                · 没提交的事务,相当于也没修改
        5 mysql后台线程定时刷新IO到磁盘
            IO没写入到磁盘中的数据,会在重启后,将数据redo加载到内存中,重新写入磁盘
    4 SQL隔离级别
        · read committed
            不会脏写/脏读,不同的两个事务,不会出现读写一半被回滚
        · repeatable read
            不会脏写/脏读/不可重复读
    5 Mysql隔离
        set SESSION transaction isolation LEVEL [REPEATABLE READ|READ COMMIT];
    6. 多个事务并发写某条数据,会加锁
    7. 数据库的写和读是读写锁,写的时候独占,读的时候共享
    8 索引 B+树
        · 在单独的建一个数据页到最小主键的链表,通过这个主键目录快速找到对应的数据页
        · 最后由于数据的增多,就会变成一棵树
    



}

;{    https http over SSL/TLS
    1 http
        · http over TCP/IP
    2 SSL Secure Sockets Layer
        收发报文不再用socket API,而是调用专门的安全接口
    3 TLS v1.2
        · SSL v3.1现在改名成TLS 最新v1.2
        · TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成,综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术
        · 混合加密
            1 非对称加密 客户端存有公钥,服务端存有私钥;
                · 现在通过非对称加密RSA通道,在客户端和服务端之间发送数据(这个数据即使被第三方抓取,也是不知道的)
            2 对称加密
                · 通过刚刚建立的加密通道,对称加密AES-CBC(DES,这属于块加密,还有流加密)的秘钥就可以加密传输到对端,两边都有AES解密秘钥
                · 现在就只用对称加密了,因为非对称加解密的耗时比对称加密多两个量级
                · 客户端向服务端发送加密的用户名/密码,服务端接收后,解密用户名和密码,同数据库作比较
}

;{    设计模式 设计模式:可复用面向对象软件的基础
    1. 创建型模式
        1. Abstract Factory (paragraph 3.1)
            客户->                              抽象工厂
                                                    抽象产品
                                                        具体工厂
                                                            具体产品
            不限定一定要有Abstract Factory, 可以直接定义一个具体工厂, 再定义继承这个具体工厂的子工厂;
            调用的时候定义一个方法,传入这个具体的子类的指针

            class MazeFactory{};
            Maze *MazeGame::CreateMaze (MazeFactory &factory){}

            class EnchantedMazeFactory : public MazeFactory{}
            class BoomedMazeFactory : public MazeFactory{}

            MazeGame mz;
            EnchantedMazeFactory enchFactory;
            BoomedMazeFactory boomFactory;
            mz.CrateMaze( enchFactory );
            mz.CrateMaze( boomFactory ); // 使用的时候,传入具体的子工厂

        2. Builder (paragraph 3.2) // 创建组件,添加到产品中
            new ConcreteBuilder
            new Director(ConcreteBuilder)
            construct: // 执行building
                ConcreteBuilder.BuildPart() // 处理请求,将部件添加到产品中
                ConcreteBuilder.GetResult() // 执行完后获取结果
                // buildpart构造代码 和 getresult表示代码分开

            1. 创建一个Buider
                class MazeBuilder{
                    public:
                        virtual void BuildMaze(){}
                        virtual void BuildRoom(){}
                        virtual void BuildDoor(){} // BuildPart
                        virtual Maze GetMaze(){} // GetResult
                    protected:
                        MazeBuilder(){}
                };

            2. Director
                Maze *MazeGame::CreateMaze (MazeBuilder &builder)
                {
                    builder.BuildMaze();
                    builder.BuildRoom();
                    builder.BuildDoor(); // building

                    return builder.GetMaze(); // GetResult
                }

            这和Abstartct Factory直接生成产品不一样, builder是在Director的控制下一步步的构造产品的,先完成build,再取回产品 
            Abstract Factory着重多个系列产品, Builder类可以差别很大
            Abstract Factory是立即返回结果的,Builder是在完成之后,由Director自己取的

        3. Factory Method (paragraph 3.3)
            Product                                     Creator
                                                            FactoryMethod()
                                                                return new Product;
            ConcreteProduct                             ConcreteCreator
                                                            FactoryMethod()
                                                                return new ConcreteProduct;

            Creator声明工厂方法,返回一个Product;ConcreteCreator重定义工厂方法,返回ConcreteProduct

            1.  定义一个Creator
                class MazeGame{
                    public:
                        Maze* CreateMaze();

                        // Factory Method
                        virtual Maze* MakeMaze(){ return new Maze;}
                        virtual Maze* MakeRoom(){ return new Room;}
                        virtual Maze* MakeWall(){ return new Wall;}
                        virtual Maze* MakeDoor(){ return new Door;}
                };

            2. 使用
                Maze* MazeGame::CreateMaze(){
                    Maze *aMaze = MakeMaze();
                    Room *r1 = MakeRoom();
                    Door *d1 = MakeDoor();

                    amze->AddRoom(r1);

                    return aMaze;
                }

            3. Abstract Factory经常用Factory Method来实现
                前者注重系列产品种类,实现细节却是Creator-ConcreteCreator::FactoryMethod{ return ConcreteProduct;}

            4. Factory Method经常在Template Method中被调用

            5. Prototypes不需要创建Ctreator子类, 但是通常要求一个针对Product类的Initialize操作。Creator使用Initialize来初始化对象。Factory不需要

        4. Prototypes (paragraph 3.4)

            client                                      Prototype
                                                            Clone()
                                                        ConcretePrototype1      ConcretePrototype2
                                                            clone()                 clone()
                                                                return copy of self     return copy of self

            class Door:public MapSite{
                public:
                    Door();
                    Door(const Door&);

                    virtual void Initialize(Room* Room*);
                    virtual Door* Clone() const;
                private:
                    Room* _room1;
                    Room* _room2;
            };

            Door::Door(const Door& other){
                _room1 = other._room1;
                _room2 = other._room2;
            }

            void Door::Initialize(Room *r1, Room *r2){
                _room1 = r1;
                _room2 = r2;
            }

            void Door::Clone() const{
                return new Door(*this);
            }

            class MazePrototypeFactory : public MazeFactory{
                public:
                    MazePrototypeFactory(Maze*, Wall*, Room*, Door*);

                    virtual Maze* MakeMaze() const;
                    virtual Room* MakeRoom(int) const;
                    virtual Wall* MakeWall() const;
                    virtual Door* MakeDoor(Room*, Room*) const;
                
                private:
                    Maze* _prototypeMaze;
                    Room* _prototypeRoom;
                    Wall* _prototypeWall;
                    Door* _prototypeDoor;
            };

            MazePrototypeFactory::MazePrototypeFactory(Maze* m, Wall* w, Room* r, Door* d)
            {
                _prototypeMaze = m;
                _prototypeRoom = r;
                _prototypeWall = w;
                _prototypeDoor = d;
            }

            Wall* MazePrototypeFactory::MakeWall() const
            {
                return _prototypeWall->Clone();
            }

            Wall* MazePrototypeFactory::MakeRoom(Room* r1, Room* r2) const
            {
                Door *door = _prototypeDoor->Clone();
                door->Initialize(r1, r2);
                return door;
            }

            MazeGame game;
            MazePrototypeFactory simpleMazeFactory(new Maze, new Wall, new Room, new Door);

            Maze *maze = game.CreateMaze(simpleMazeFactory);

        5. singleton (paragraph 3.5)
            class Singleton{
                public:
                    static Singleton* Instance();
                protected:
                    Singleton();
                private:
                    static Singleton* _instance;
            };

            Singleton* Singleton::_instance = 0;

            Singleton* Singleton::Instance()
            {
                if (_instance == 0)
                    _instance = new Singleton;
                return _instance;
            }

            对于多种类型的子类创建:
            Singleton* Singleton::Instance()
            {
                if (_instance == 0)
                    _instance = new Singleton;
                else{
                    char *sz = getenv("MAZESTYLE");
                    
                    if (!strcmp(sz, "bombed"))
                    {
                        _instance = new BombedMazeFactory;
                    }else if (!strcmp(sz, "enchanted"))
                    {
                        _instance = new EnchantedMazeFactory;
                    }
                }
                return _instance;
            }

    2. 结构型模式 // 处理类或对象的组合 (paragraph 4)
        · 涉及如何组合类和对象以获得更大的结构

        1. ADAPTER (paragraph 4.1)
            · 别名: 包装器Wrapper
            · 意图: 将一个类的接口转换成客户希望的另外一个接口
            · 结构(paragraph 4.1.5)
                1. 多继承
                    client -> Target                Adaptee
                                request()               SpecifiedRequest()
                                    ↑                     ↑
                                            Adapter
                                                request()
                                                    Adaptee::SpecifiedRequest()

                2. 对象组合
                    client -> Target                Adaptee
                                request()               SpecifiedRequest()
                                    ↑                     ↑
                                            Adapter
                                                Adaptee adaptee;
                                                request()
                                                    adaptee.SpecifiedRequest()

                原有的类Target,原有的接口:request()
                新的类Adaptee, 新的接口:SpecifiedRequest()
                Adapter同时继承两个类或者内含新类的实例,经由原来的类的接口调用到新的类或者对象的接口

            代码示例:
                1. 多继承
                    class Shape{
                        public:
                            Shape();
                            virtual void BoundingBox();
                            virtual Manipulator* CreateManipulator();
                    };

                    class TextView{
                        public:
                            TextView();
                            void GetOrigin();
                            void GetExtent();
                            virtual bool IsEmpty();
                    };

                    // adapter 公开继承原有的接口,私有继承新接口(只能调用新类提供的函数访问私有变量)
                    class TextShape: public Shape, private TextView{
                        public:
                            TextShape();
                            virtual void BoundingBox();
                            virtual Manipulator* CreateManipulator();
                            virtual bool IsEmpty();
                    };

                    void TextShape::BoundingBox(){
                        GetOrigin(); // 老接口调用新接口
                        GetExtent();
                    }

                    bool TextShape::IsEmpty(){
                        return TextView::IsEmpty(); // 显示调用新接口
                    }

                    Manipulator* TextShape::CreateManipulator(){
                        return new TextManipulator(this);
                    }

                2. 类和对象组合
                    class TextShape:public Shape{
                        public:
                            TextShape(TextView*);
                            virtual void BoundingBox();
                            virtual Manipulator* CreateManipulator();
                            virtual bool IsEmpty();
                        private:
                            TextView* _text; // 内含新类对象
                    };

                    void TextShape::BoundingBox(){
                        _text->GetOrigin(); // 老接口调用新接口
                        _text->GetExtent();
                    }

                    bool TextShape::IsEmpty(){
                        return _text->IsEmpty(); // 显示调用新接口
                    }

            · 相关模式 (paragraph 4.1.12)
                Bridge, Decorator, Proxy 

        2. BRIDGE (paragraph 4.2)
            · 意图
                将抽象部分和实现部分分离,使他们可以独立的变化
            · 别名
                Handle/Body
            · 代码示例:
                class Window{
                    public:

                    protected:
                        WindowsImp *GetWindowImp(){
                            if (_imp == 0){
                                _imp = WindowsSystemFactory::Instance()->MakeWindowImp();
                            }
                            return _imp;
                        }
                    
                    private:
                        WindowsImp *_imp;
                };

                class WindowsImp{ //通用的抽象接口
                    public:
                };

                class XWindowImp:public WindowsImp{}
                class PMWindowImp:public WindowsImp{} // 具体的实现Imp接口

                class ApplicationWindow:public Window{
                    // 公开继承带有基本操作的类
                };

                class IconWindow:public Window{
                    // 公开继承带有基本操作的类
                    public:
                        virtual void DrawContents(){
                            // 获取windowsImp类中的接口
                            WindowsImp *imp = GetWindowImp(); // 使用Abstract Factory生成需要的Imp接口
                            if (imp!=0)
                            {
                                imp->DeviceBitmap();
                            }
                        }
                };
            · 相关模式
                Abstract Factory可以用来创建和配置一个特定的bridge模式
                Adapter 模式用来帮助无关的类协同工作,通常在系统设计完成后被使用
                Bridge 模式在系统开始时就被使用,使得抽象接口和实现部分可以独立进行改变
        
        4.3 COMPOSITE // 描述部分-整体 或者容器层次结构

            代码示例:
                class Equipment{ // component

                };

                class FloppyDisk : public Equipment{ // leaf

                };

                class CompositeEquipment:public Equipment{ // composite
                    public:
                        virtual Currency NetPrice(); // component基本属性
                        virtual void Add(); //  管理组件的接口
                        virtual void Remove();

                    private:
                        List<Equipment*> _equipment; // 递归的子类容器
                };

                class Chassis: public CompositeEquipment{
                };

                Cabinet* cabinet = new Cabinet("PC cabinet");
                Chassis* chassis = new Chassis("PC chassis");

                cabinet->add(Chassis); // cabinet中装入chassis

                Bus* bus = new Bus("PC bus");
                bus->Add(new Card("16MB"));

                chassis->Add(bus);
                chassis->Add(new FloppyDisk("3.5in floppy"));
                // 最后cabinet就组装好了
            · 相关模式
                Responsibility of chain 用于部件和父部件
                Decorator 通常与Composite一起使用
                Flyweight 允许共享组件,但不能再引用父部件
                Iterator可以遍历Composite
                Vistor将分布在Comosite和Leaf类中的操作和行为局部化
        4.4 DECORATOR
            1. 意图
                动态的给一个对象添加一些额外的职责
            2. 别名
                包装器wrapper
            3. 代码示例
                class VisualComponent{
                    public:
                        virtual void Draw();
                        virtual void Resize();
                };
                class Decorator: public VisualComponent{
                    public:
                        virtual void Draw(){
                            _component->Draw();
                        }
                        virtual void Resize(){
                            _component->Resize();
                        }
                    private:
                        VisualComponent *_component;
                        // Decorator装饰_component
                };

                class BorderDecorator: public Decorator{
                    public:
                        virtal void Draw();
                };

                VisualComponent *cc = new BorderDecorator(TextView);
                cc->Draw();// 实际调用的是BorderDecorator::Draw();
            
            12. 相关模式
                Adapter:重新设置一个接口;
                    Decorator:不改变接口,只是修改职责
                Composite:主要是聚合一系列对象,描述部分和整体或者容器
                Strategy:改变对象的内核
                    Decorto:改变对象的外表
        
        4.5 FACADE
            1. 意图
                为子系统中的一组接口提供一个一致的界面
            2. 代码示例
                class Compiler{
                    public:
                        Compiler();
                        virtual void Compile(istream&, BytecodeStream&);
                        // facade
                };

                void Compiler::Compile(istream& is, BytecodeStream& bs){
                    // subsystem
                    Scanner scanner(is);
                    ProgramNodeBuilder builder;
                    Parser parser;

                    parser.Parse(scanner, builder);

                    RISCCodeGenerator generator(bs);
                    ProgramNode *parseTree = builder.GetRootNode();
                    parseTree->Traverse(generator);
                };
            
            3. 相关模式
                Abstract Factory: 可以和Facade一起使用以提供一个接口,这个接口用来生成子系统
                Facade对象通常属于Singleton

        4.6 FLYWEIGHT
            1. 意图
                运用共享技术有效地支持大量细粒度的对象
            2. 举例
                创建有限的字符对象集合,以及描述字符对象的所在场景的状态对象集合
                这里的字符对象集合就是共享对象,用状态来描绘
        
        4.7 PROXY
            1. 意图
                为其他对象提供一种代理以控制对这个对象的访问
            2. 别名
                Surrogate
            3. 三种类型
                1. remote proxy
                2. virtual proxy
                3. protection proxy
        
        4.8 结构型模式讨论
            Adapter在类设计之后,为了解决两个类不兼容
            Bridge在类设计之前,就要将抽象的接口独立出来

            Decorator别名包装器Wrapper,实际调用的是包装器的接口
            Composite描述部分-整体结构或者容器结构,能够从顶层遍历所有的leaf
            proxy提供一个代理,代理决定是否访问具体实现,Decorator提供具体实现

    3. 行为模式 (paragraph 5)
        · 行为模式涉及到算法和对象间职责的分配
        · 行为模式不仅描述类和对象,还描述他们之间的通信模式

        5.1 CHAIN OF RESPONSIBILITY
            1. 意图
                使多个对象都有机会处理请求,从而避免请求的发送者和接收者之间的耦合关系.将这些对连成一条链,并沿着这条链传递该要求,直到有一个对象处理它为止
            2. 代码示例
                Application* app = new Application;
                Dialog* dialog = new Dialog(app);
                Button* button = new Button(dialog);

                button->HandleHelp();
                // button->dialog->app, 串联起来,一次查找

            3. 相关模式
                CHAIN OF RESPONSIBILITY(职责链)通常与Composite一起使用
        5.2 COMMAND - 对象行为模式
            1. 意图
                将一个请求封装为一个对象的
            2. 别名
                Action
            3. 类似回调函数,先注册函数,将命令发给指定对象后,再回调这个函数
                Command发送给指定对象后,接收者执行Command的execute()接口
        
        5.3 INTERPRETER - 类行为模式
            1. 意图
                给定一个语言,定义它的文法的一种表示,再定义一个解释器,再由解释器使用该表示来解释语言中的句子
            
            2. 复杂的文法不如用语法分析程序
        
        5.4 ITERATOR - 对象行为模式
            1. 意图
                提供一种方法顺序访问一个聚合对象中各个元素,而又不需要暴露该对象的内部表示
            2. 别名
                游标 Cursor
            3. 动机
                将对列表的访问和遍历从列表对象中分离出来并放入一个iterator中。
            4. 代码示例
                template <class Item>
                class List{
                    public:
                };

                template <class Item>
                class Iterator{
                    public:
                };

                template <class Item>
                class ListIterator:public Iterator{
                    public:
                        ListIterator(const List<Item>* aList):_list(alist), _current(0){}
                        virtual void First(){
                            _current = 0;
                        }

                        virtual void Next(){
                            _current++;
                        }

                        virtual bool IsDone() const{
                            return _current >=_list->count;
                        }

                        virtual Item CurrentItem() const{
                            if (IsDone()){
                                throw IteratorOutOfBounds;
                            }
                            return _list->Get(_current);
                        }
                    private:
                        const List<Item>* _list;
                        long _current;
                        // 存储List和列表当前位置的索引
                };

                void PrintEmployees (Iterator<Employee*> &i){
                    for(i.First(); !i.IsDone(); i.Next()){
                        i.CurrentItem()->print();
                    }
                }

                List<Employee*> *employees;
                LitIterator<Employee*> forward(employees);
                ReverseListIterator<Employee*> backward(employees);

                PrintEmployees(forward);
                PrintEmployees(backward);

                // 优化,隐藏手动实例化Iterator,使用List来生成
                template <class Item>
                Iterator<Item>* List<Item>::CreateIterator() const{
                    return new ListIterator<Item>(this);
                }

                AbstractList<Employee*>* employees;
                ...
                Iterator<Employee*>* iterator = employees->CreateIterator(employees);
                PrintEmployees(*iterator);
                delete iterator; // 为了自动释放,使用智能指针
        5.5 MEDIATOR - 对象行为模式
            1. 意图
                用一个中介对象来封装一系列的对象交互
            2. 实际就是封装一个时序图的接口对象出来
            3. 代码示例
                class DialogDirector{
                    public:
                        virtual void ShowDialog();
                        virtual void WidgetChanged(Widget*) = 0;
                    protected:
                        virtual void CreateWidgets() = 0;
                };

                class Widget{
                    public:
                        Widget(DialogDirector *);
                        virtual void Changed(){
                            _director->WidgetChanged(this);
                        }

                        virtual void HandleMouse(MouseEvent& event);
                    private:
                        DialogDirector *_director;
                };
            4. 相关模式
                Facade是提供一个接口驱动系统
                Mediator是提供接口,供各个子系统通信
        
        5.6 MEMENTO - 对象行为模式
            1. 意图
                在不破坏封装性的前提下,捕获一个对象的内部状态,并在该对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态
            2. 别名
                Token
            3. 代码示例
                class State;
                class Originator{
                    public:
                        Memento* CreateMemento();
                        void SetMemento(const Memento);
                    private:
                        State *_state;
                };

                class Memento{
                    public:
                        virtual ~Memento();
                    private:
                        friend class Origineator; // 只允许Originator访问Memento
                        Memento();
                        void SetState(State*);
                        State* GetState();

                        State* _state;
                };

                class Graphic;
                class MoveCommand{
                    public:
                        MoveCommand(Graphic* target, const Point& delta);
                        void Execute(){
                            ConstraintSolver* solver = ConstraintSolver::instance();
                            _state = solver->CreateMemento();
                            _target->Move(_delta);
                            solver->Solve();
                            // 备忘录的私有成员只有原发器能访问
                            // 原发器执行操作
                        }
                        void Unexecute(){
                            ConstraintSolver* solver = ConstraintSolver::instance();
                            _target->Move(-_delta);
                            solver->SetMemento(_state);
                            solver->Solve();
                            // 备忘录的私有成员只有原发器能访问
                            // 原发器先恢复状态, 在执行操作
                        }
                    private:
                        ConstraintSolverMemento* _state;
                        Point _delta;
                        Graphic* _target;
                };

                class ConstraintSolver{ // 原发器
                    public:

                };

                class ConstraintSolverMemento{ // 备忘录
                    private:
                        friend class ConstraintSolver;
                        
                };
            4. 相关模式
                Command:原发器让备忘录保存状态
                Iterator:让备忘录迭代
        
        5.7 OBSERVER - 对象行为模式
            1. 意图
                在对象间建立一对多的依赖关系,当一个对象的状态发生变化,所有依赖他的对象都会发生变化
            2. 别名
                Dependents, Publish-Subscribe
            3. 序列图
                aConcretSubject     aConcretObserver    aConcretObserver
                    notify()
                    update()------------
                                        GetState()
                    update()------------
                                                            GetState()
            4. pull/push模型
                pull 被动的让observer来获取状态更新
                push 主动的发出消息,不管observer是否需要

            5. ChangeManager (Mediator\Singleton)
                将序列图的操作抽象出来
            6. 代码
                class Subject;
                class Observer{
                    public:
                        virtual ~Observer();
                        virtual void Update(Subject* theChangedSubject) = 0;
                    protected:
                        Observer();
                };

                class Subject{
                    public:
                        virtual ~Subject();
                        virtual void Attach(Observer* o)
                        {
                            _observer->Append(o);
                        }
                        virtual void Detach(Observer* o){
                            _observer->Remove(o);
                        }
                        virtual void Notify(){
                            ListIterator<Observer*> i(_observer);
                            for (i.First(); !i.IsDone(); i.Next()){
                                i.CurrentItem()->Update(this);
                            }
                        }// 目标发生变化,触发通知,让observer update自身的状态
                    protected:
                        Subject();
                    private:
                        List<Observer*> *_observer;
                };

                class ClockTimer:public Subject{
                    public:
                        ClockTimer();
                        virtual int GetHour();
                        virtual int GetMinute();
                        virtual int GetSecond();
                        void Tick(){
                            Notify();// 每间隔一段时间就通知observer获取时间更新状态
                        }
                };

                // DigitalClock 继承Widget的图形功能,继承Observer的Notify,获取时间的接口
                class DigitalClock: public Widget, public Observer{
                    public:
                        DigitalClock(ClockTimer* s){
                            _subject = s;
                            _subject->Attach(this);
                            // 将observer加入到subject中
                        }
                        virtual ~DigitalClock(){
                            _sub->Detach(this);
                        }
                        virtual void Update(Subject* theChangedSubject){
                            if (theChangedSubject == _subject){
                                Draw();// update绘制时钟图形
                            }
                        }
                        virtual void Draw(){
                            int hour = _subject->GetHour();
                            int minute = _subject->GetMinute
                        }
                    private:
                        ClockTimer* _subject;
                };

                ClockTimer*timer = new ClockTimer; // subject notify observer
                DigitalClock* DigitalClock = new DigitalClock(timer); // observer 进行重绘

                MVC就是observer模式
            6. 相关模式
                Mediator:ChangeManager抽象序列图中的更新语义
                Singleton:ChangeManager是唯一全局可访问的
        
        5.8 STATE - 对象行为型模式
            1. 意图
                允许一个对象在其内部状态改变时改变他的行为。对象看起来似乎修改了他的类
            2. 别名
                状态对象(Object For State)
            3. 代码示例
                class TCPOctetStream;
                class TCPState;
                class TCPConnection{
                    public:
                        TCPConnection(){
                            _state = TCPClosed::Instance();
                        }
                        void ActiveOpen(){
                            _state->ActiveOpen(this);
                        }
                        void PassiveOpen(){
                            _state->PassiveOpen(this);
                        }
                        void Close(){
                            _state->Close(this);
                        }
                        void Send();
                        void Acknowledge(){
                            _state->Acknowledge(this);
                        }
                        void Synchronize(){
                            _state->Synchronize(this);
                        }
                        void ProcessOctet(TCPOctetStream*);
                    private:
                        friend class TCPState;
                        // 友元让TCPState能访问TCPConnection的私有成员
                        void ChangeState(TCPState* s){
                            _state = s;
                        }
                        TCPState* _state; // 保持各个TCP状态的引用
                };

                class TCPState{
                    public:
                        virtual void Transmit(TCPConnection*, TCPOctetStream*);
                        virtual void ActiveOpen(TCPConnection*);
                        virtual void PassiveOpen(TCPConnection*);
                        virtual void Close(TCPConnection*);
                        virtual void Synchronize(TCPConnection*);
                        virtual void Acknowledge(TCPConnection*);
                        virtual void Send(TCPConnection*);
                    protected:
                        void ChangeState(TCPConnection* t, TCPState* s){
                            t->ChangeState(s);
                        }
                        // TCPState可以修改TCPConnection中的数据,改变连接状态
                };

                在完成与状态相关的工作后,这些操作调用ChangeState来改变TCPConnection的状态。TCPConnection本身对TCP连接协议一无所知,是由TCPState子类来定义TCP中的每一个状态转换和动作
            4. 相关模式
                状态对象都是SingleTon
                FlyWeight解释了何时以及怎样共享状态对象,共享内部的有限对象,外部记录这些对象的状态
        
        5.9 STRATEGY - 对象行为模式
            1. 意图
                定义一系列的算法,把他们一个个的封装起来,并使他们可以互相替换
            2. 别名
                Policy
            3. 相关模式
                Flyweight:Strategy对象经常是很好的轻量级对象
        
        5.10 TEMPLATE METHOD - 类行为型模式
            1. 意图
                定义一个算法的骨架,将一些步骤延迟到子类中。使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。
            2. 代码
                void View::Display(){
                    SetFocus();
                    DoDisplay();
                    ResetFocus();
                }

                void MyView::DoDisplay(){}
            3. 相关模式
                Factory Method:经常被Tamplate Method调用,如上面的MyView::DoDisplay()
                Strategy:Tamplate Method使用继承来改变算法的一部分;Strategy改变整个算法
             
        5.11 VISTOR - 对象行为型模式
            1. 意图
                对一批元素进行不同处理的时候,封装不同的vistor类,对不同的元素做不同vistor的处理
        
        5.12 行为模式的讨论
            5.12.1 封装变化
                1. 当一个程序的某个方面的特征经常发生改变时,这些模式就定义一个封装这个方面的对象
                2. 这些模式通常定义一个抽象类来描述这些封装变化的对象,并且通常该模式依据这个对象来命名,例如:
                    一个strategy对象封装一个算法
                    一个state对象封装一个与状态有关的行为
                    一个mediator对象封装对象间的协议
                    一个iterator对象封装访问和遍历一个聚集对象中的各个构件的方法

            5.12.2 对象作为参数
                1. 一个vistor对象是一个多态的Accept操作的参数
                2. 其他模式定义一些可作为令牌到处传递的对象,这些对象将在稍后被调用
                    1. Command 中,令牌代表一个请求
                    2. Memento 中,代表一个对象在某个特定时刻的内部状态
                    3. Command会用到多态,Memento作为对象传入使用
            5.12.3 通信应该被封装还是被分布
                1. mediator和observer互为竞争关系,后者在Observer和subject对象间通信,前者则是封装了对象间的通信
            5.12.4 对发送者和接收者解耦
                1. command模式使用一个command对象来定义一个发送者和一个接收者之间的绑定关系,从而支持解耦
                2. observer通过定义一个接口来通知目标中的发生的变化,从而将发送者和接收者解耦;当对象间有数据依赖,最好使用observer模式
                3. mediator让对象通过一个mediator对象间接的相互引用,从而对他们解耦;一个mediator对象为各对象间的请求提供路由并集中他们的通信
                4. chain of responsibility通过沿着一个潜在接收者链传递请求而将发送者和接收者解耦
            5.12.5 总结
                1. 另外一些例外情况,各个行为设计模式之间是相互补充和相互加强的关系。
                    1. chain of responsibility模式中可能包含至少一个template method的应用,该template method可以使用原语确定该对象是否应处理该请求并选择应转发的对象;
                        command: 可以被用作请求对象
                    2. Interpreter可以使用state模式定义语法分析上下文
                    3. Iterator可以遍历一个聚合,vistor可以作为参数对这些元素进行一个操作
                2. 行为模式也能和其他模式协同工作
                    1. composite可以使用vistor对符合的成分进行一些操作;chain of responsibility可以通过composite通过父类访问某些全局属性;decorator 可以对composite的某些部分进行改写;它可以使用observer将一个对象结构和另一个对象结构联系起来;它可以使用state模式使一个构件在状态改变时改变自身的行为;composite可以使用builder创建
}

;{    网络数据包收发流程

    1. NAPI : 采用 中断 + 轮询 的方式:mac收到一个包来后会产生接收中断,但是马上关闭。
        直到收够了netdev_max_backlog个包(默认300), 或者收完mac上所有包后,才再打开接收中断
        通过sysctl来修改 net.core.netdev_max_backlog
        或者通过proc修改 /proc/sys/net/core/netdev_max_backlog

    2. 每个网络设备(MAC层)都有自己的net_device数据结构,这个结构上有napi_struct
        每当收到数据包时,网络设备驱动会把自己的napi_struct挂到CPU私有变量上。
        这样在软中断时,net_rx_action会遍历cpu私有变量的poll_list
        执行上面所挂的napi_struct结构的poll钩子函数,将数据包从驱动传到网络协议栈。
        ---------------------
        |网卡    ->  CPU    |
        |soft_data|soft_data|
        |poll_list|poll_list|

    3. 初始化网络相关的全局数据结构,并挂载处理网络相关软中断的钩子函数
        start_kernel()
            --> rest_init()
            --> do_basic_setup()
            --> do_initcall
            --> net_dev_init
            ```
                定义:
                subsys_initcall(net_dev_init);
                #define subsys_initcall(fn) __define_initcall("4",fn,4)
                扩展到:
                static initcall_t __initcall_net_dev_init4 __attribute_used__ __attribute__((__section__(".initcall" "4" ".init"))) = net_dev_init

                调用:
                for (call = __initcall_start; call < __initcall_end; call++) 
                    result = (*call)();
            ```

            ```
            	for_each_possible_cpu(i) {
                    struct softnet_data *queue;

                    queue = &per_cpu(softnet_data, i); // CPU私有变量, per_cpu来获取
                    ```
                        DEFINE_PER_CPU(struct softnet_data, softnet_data);
                        #define DECLARE_PER_CPU(type, name) extern __percpu __attribute__((section(.data..percpu))) __typeof__(type) name
                        在.data..percpu区定义一个softnet_data结构
                    ```
                    skb_queue_head_init(&queue->input_pkt_queue); //初始化sk_buff接收队列
                    queue->completion_queue = NULL; //初始化sk_buff处理完成队列
                    INIT_LIST_HEAD(&queue->poll_list); //

                    queue->backlog.poll = process_backlog;
                    queue->backlog.weight = weight_p;
                }

                netdev_dma_register(); // 注册DMA

                open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL); //在软中断上挂网络发送handler
	            open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL);  //在软中断上挂网络接收handler
            ```

    4. 加载网络设备的驱动
        在网络设备驱动中创建net_device数据结构,并初始化其钩子函数 open(),close() 等
        挂载TSEC的驱动的入口函数是 gfar_probe
        ```
        /* Structure for a device driver */
        static struct platform_driver gfar_driver = {
            .probe = gfar_probe,
            .remove = gfar_remove,
            .driver	= {
                .name = "fsl-gianfar",
            },
        };
        ```

        ```
        static int gfar_probe(struct platform_device *pdev)
        {
            struct net_device *dev = NULL;
            dev = alloc_etherdev(sizeof (*priv)); // 创建一个net_dev
            if (einfo->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) 
            {
                priv->interruptTransmit = platform_get_irq_byname(pdev, "tx");
                priv->interruptReceive = platform_get_irq_byname(pdev, "rx"); //从设备树里获取设备中断号
                priv->interruptError = platform_get_irq_byname(pdev, "error");
            }

            dev->open = gfar_enet_open;
            dev->hard_start_xmit = gfar_start_xmit;
            dev->tx_timeout = gfar_timeout; // 设置dev的回调函数
            netif_napi_add(dev, &priv->napi, gfar_poll ,GFAR_DEV_WEIGHT); //软中断里会调用poll钩子函数

        };
        ```

    5. 启用网络设备
        5.1 用户调用ifconfig等程序,然后通过ioctl系统调用进入内核
            socket的ioctl()系统调用
            --> sock_ioctl()
            --> dev_ioctl()                              //判断SIOCSIFFLAGS
            --> __dev_get_by_name(net, ifr->ifr_name)  //根据名字选net_device
            --> dev_change_flags()                  //判断IFF_UP
            --> dev_open(net_device)             //调用open钩子函数

            对于TSEC来说,挂的钩子函数是 gfar_enet_open(net_device)

        5.2 在网络设备的open钩子函数里,分配接收bd,挂中断ISR(包括rx、tx、err)
            对于TSEC来说gfar_enet_open
                --> 给Rx Tx Bd 分配一致性DMA内存
                --> 把Rx Bd的“EA地址”赋给数据结构,物理地址赋给TSEC寄存器
                --> 把Tx Bd的“EA地址”赋给数据结构,物理地址赋给TSEC寄存器
                --> 给 tx_skbuff 指针数组 分配内存,并初始化为NULL
                --> 给 rx_skbuff 指针数组 分配内存,并初始化为NULL

                --> 初始化Tx Bd
                --> 初始化Rx Bd,提前分配存储以太网包的skb,这里使用的是一次性dma映射
                (注意: #define DEFAULT_RX_BUFFER_SIZE  1536 保证了skb能存一个以太网包)
                --> startup_gfar
                    txbdp = priv->tx_bd_base;
                    rxbdp = priv->rx_bd_base;
                        ```
                    	for (i = 0; i < priv->rx_ring_size; i++) {
                            struct sk_buff *skb = NULL;
                            rxbdp->status = 0;
                            skb = gfar_new_skb(dev, rxbdp); //这里真正分配skb,并且初始化rxbpd->bufPtr, rxbdpd->length
                            priv->rx_skbuff[i] = skb; //RXBDP和 SK_BUFF映射起来
                            rxbdp++;
                        }
                        rxbdp--;
                        rxbdp->status |= RXBD_WRAP; // 给最后一个bd设置标记WRAP标记
                        ```

                --> 注册TSEC相关的中断handler: 错误,接收,发送
                    request_irq(priv->interruptError, gfar_error , 0, "enet_error", dev)
                    request_irq(priv->interruptTransmit, gfar_transmit , 0, "enet_tx", dev) // 注册TX中断号, 包发送完
                    request_irq(priv->interruptReceive, gfar_receive , 0, "enet_rx", dev) // 注册RX中断号, 包接收完

                -->gfar_start(net_device)
                    // 使能Rx、Tx
                    // 开启TSEC的 DMA 寄存器
                    // Mask 掉我们不关心的中断event

    6. 中断里接收以太网包
        TSEC的RX已经使能了,网络数据包进入内存的流程为:
            网线 --> Rj45网口 --> MDI 差分线
            --> bcm5461(PHY芯片进行数模转换) --> MII总线
            --> TSEC的DMA Engine 会自动检查下一个可用的Rx bd
            --> 把网络数据包 DMA 到 Rx bd 所指向的内存,即skb->data -> 触发网卡中断

        6.1 网卡驱动的接收中断处理函数
            irqreturn_t gfar_receive(int irq, void *dev_id)
            {
                if (netif_rx_schedule_prep(dev, &priv->napi)) {
                    tempval = gfar_read(&priv->regs->imask);
                    tempval &= IMASK_RX_DISABLED;
                    gfar_write(&priv->regs->imask, tempval);

                    __netif_rx_schedule(dev, &priv->napi);
                        -> list_add_tail(&n->poll_list, &__get_cpu_var(softnet_data).poll_list); // 将DMA过来的数据加到cpu私有变量中
                        -> __raise_softirq_irqoff(NET_RX_SOFTIRQ); //主动触发NET_RX_SOFTIRQ中断
                }
            }

        6.2 网络接收 软中断net_rx_action
            static void net_rx_action(struct softirq_action *h)
            {
                struct list_head *list = &__get_cpu_var(softnet_data).poll_list;
                while (!list_empty(list)) {
                    struct napi_struct *n;
                    n = list_entry(list->next, struct napi_struct, poll_list); // 取出poll_list
                    if (test_bit(NAPI_STATE_SCHED, &n->state))
                    {
                        work = n->poll(n, weight); // 调用驱动poll()函数
                        ```
                            static int gfar_poll(struct napi_struct *napi, int budget)
                            {
                                struct gfar_private *priv = container_of(napi, struct gfar_private, napi);
	                            struct net_device *dev = priv->dev;

                                int howmany = gfar_clean_rx_ring(dev, budget); // 根据dev的rx bd,获取skb并送入协议栈,返回处理的skb的个数,即以太网包的个数

                                // 下面这个判断比较有讲究的
                                // 收到的包的个数小于budget,代表我们在一个软中断里就全处理完了,所以打开 rx硬中断
                                // 要是收到的包的个数大于budget,表示一个软中断里处理不完所有包,那就不打开 rx硬中断 ,
                                // 此次软中断的下一轮循环里 再接着处理,直到包处理完(即howmany rx硬中断
                                if (howmany < budget) {
		                            netif_rx_complete(dev, napi); //del form poll_list
                                    gfar_write(&priv->regs->rstat, RSTAT_CLEAR_RHALT);
                                    //打开 rx 硬中断,rx 硬中断是在gfar_receive()中被关闭的
                                    gfar_write(&priv->regs->imask, IMASK_DEFAULT);
                                }
                            }

                            int gfar_clean_rx_ring(struct net_device *dev, int rx_work_limit)
                            {
                                struct rxbd8 *bdp;
                                struct gfar_private *priv = netdev_priv(dev);
                                /* Get the first full descriptor */
                                bdp = priv->cur_rx;

                                while (!((bdp->status & RXBD_EMPTY) || (--rx_work_limit < 0))) {
                                    skb = priv->rx_skbuff[priv->skb_currx];
                                    if (!(bdp->status & (RXBD_LARGE | RXBD_SHORT | RXBD_NONOCTET
                                    | RXBD_CRCERR | RXBD_OVERRUN | RXBD_TRUNCATED))) {
                                        pkt_len = bdp->length - 4; //从length中去掉以太网包的FCS长度
                                        gfar_process_frame(dev, skb, pkt_len);
                                        dev->stats.rx_packets++;
                                        dev->stats.rx_bytes += pkt_len;
                                    }

                                    dev->last_rx = jiffies;

                                    /* Add another skb for the future */
                                    skb = gfar_new_skb(dev, bdp);
                                    priv->rx_skbuff[priv->skb_currx] = skb;

                                    /* Update to the next pointer */
                                    if (bdp->status & RXBD_WRAP)
                                        bdp = priv->rx_bd_base;
                                    else
                                        bdp++;

                                    		/* update to point at the next skb */
                                    priv->skb_currx = (priv->skb_currx + 1) & RX_RING_MOD_MASK(priv->rx_ring_size);
                                }

                                priv->cur_rx = bdp;
                            }

                            /* gfar_process_frame() -- handle one incoming packet if skb * isn't NULL.  */
                            static int gfar_process_frame(struct net_device *dev, struct sk_buff *skb, int length)
                            {
                                skb->protocol = eth_type_trans(skb, dev); //确定网络层包类型,IP、ARP、VLAN等等
                                ret = RECEIVE(skb); //netif_receive_skb(skb)
                            }
                        ```
                    }
                }

                //以太网的FCS会在网卡中断 (如gfar_clean_rx_ring) 中忽略掉
                /* Remove the FCS from the packet length */
                pkt_len = bdp->length - 4;

                //至于填充数据,是在协议栈中被忽略掉的,比如ip协议 ip_rcv()
                /* Our transport medium may have padded the buffer out. Now we know it
                * is IP we can trim to the true length of the frame.
                * Note this now means skb->len holds ntohs(iph->tot_len).
                */
                if (pskb_trim_rcsum(skb, len)) {
                    IP_INC_STATS_BH(IPSTATS_MIB_INDISCARDS);
                    goto drop;
                }
            }

    7. 进入二层协议处理函数
        netif_receive_skb
            list_for_each_entry_rcu(ptype, &ptype_all, list) { // 所有数据报文在这里都不会处理
                if (!ptype->dev || ptype->dev == skb->dev) {
                    if (pt_prev)
                        ret = deliver_skb(skb, pt_prev, orig_dev);
                    pt_prev = ptype;
                }
            }

            #if defined(CONFIG_BRIDGE) || defined (CONFIG_BRIDGE_MODULE)
                skb = handle_bridge(skb, &pt_prev, &ret, orig_dev); // bridge的skb->pkt_type == PACKET_HOST, PACKET_OTHER_HOST
                ```
                        static int __init br_init(void){
                            br_handle_frame_hook = br_handle_frame;
                        }
                ```
                if (!skb)
                    goto out;
            #endif

            #if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
                skb = handle_macvlan(skb, &pt_prev, &ret, orig_dev); // bridge的skb->pkt_type == PACKET_BROADCAST, PACKET_MULTICAST, PACKET_HOST
                ```
                    static int __init macvlan_init_module(void)
                    {
                        macvlan_handle_frame_hook = macvlan_handle_frame;
                    }
                ```
                if (!skb)
                    goto out;
            #endif
            type = skb->protocol;
            list_for_each_entry_rcu(ptype, &ptype_base[ntohs(type)&15], list) {
                ```
                    static struct list_head ptype_base[16] __read_mostly;	/* 16 way hashed list */
                    static int __init inet_init(void)
                    {
                        dev_add_pack(&ip_packet_type);
                        ```
                            void dev_add_pack(struct packet_type *pt)
                            {
                                hash = ntohs(pt->type) & 15;
                                list_add_rcu(&pt->list, &ptype_base[hash]);
                            }
                        ```
                        ```
                            static struct packet_type ip_packet_type = {
                                .type = __constant_htons(ETH_P_IP),
                                .func = ip_rcv,
                                .gso_send_check = inet_gso_send_check,
                                .gso_segment = inet_gso_segment,
                            };
                        ```
                    }
                ```
                if (ptype->type == type &&
                    (!ptype->dev || ptype->dev == skb->dev)) {
                    if (pt_prev)
                        ret = deliver_skb(skb, pt_prev, orig_dev);
                    pt_prev = ptype;
                }

                if (pt_prev) {
                    ret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev); //ip_rcv arp_rcv
                }
            }
}

;{    伪文件系统
    1. socketfs
        1.1 sys_socket
            ```
                retval = sock_create(family, type, protocol, &sock);
                retval = sock_map_fd(sock); // sock - file - fd 联系在一起
                    int err = sock_attach_fd(sock, newfile);
                        SOCK_INODE(sock)->i_fop = &socket_file_ops; //inode_ops - sock_file_ops
                        ```
                            static const struct file_operations socket_file_ops = {
                                .owner =	THIS_MODULE,
                                .llseek =	no_llseek,
                                .aio_read =	sock_aio_read,
                                .aio_write =	sock_aio_write,
                                .poll =		sock_poll,
                                .unlocked_ioctl = sock_ioctl,
                            #ifdef CONFIG_COMPAT
                                .compat_ioctl = compat_sock_ioctl,
                            #endif
                                .mmap =		sock_mmap,
                                .open =		sock_no_open,	/* special open code to disallow open via /proc */
                                .release =	sock_close,
                                .fasync =	sock_fasync,
                                .sendpage =	sock_sendpage,
                                .splice_write = generic_splice_sendpage,
                            };

                            open
                            -> sys_open
                            -> do_sys_open
                            -> do_filp_open
                            -> nameidata_to_filp
                            -> __dentry_open
                            	-> if (!open && f->f_op)
                                    open = f->f_op->open;
                                if (open) 
                                    error = open(inode, f);

                            static struct file *__dentry_open(struct dentry *dentry, struct vfsmount *mnt,
                                                int flags, struct file *f,
                                                int (*open)(struct inode *, struct file *))
                            {
                                f->f_op = fops_get(inode->i_fop); // file_ops - inode_ops - socket_file_ops
                            }
                        ```
            ```
    2. procfs
        2.0 open/read/write参考socketfs的dentry_open
        2.1 初始化
            start_kernel()
                ->proc_root_init()
                    ->register_filesystem(&proc_fs_type);
                    ```
                        static struct file_system_type proc_fs_type = {
                            .name		= "proc",
                            .get_sb		= proc_get_sb,
                            .kill_sb	= proc_kill_sb,
                        };
                    ```
                    ->proc_sys_init()
                        -> proc_sys_root->proc_iops = &proc_sys_inode_operations;
	                    -> proc_sys_root->proc_fops = &proc_sys_file_operations;
                    ->proc_tty_init()
                        -> entry->proc_fops = &proc_tty_drivers_operations;

        2.2 挂载文件系统(/proc根目录)
            mount -t proc proc /proc
            ->.get_sb		= proc_get_sb
            -> proc_fill_super(sb);
                -> s->s_op = &proc_sops; //主要是通过proc_fill_super填充超级块实例
                -> root_inode = proc_get_inode(s, PROC_ROOT_INO, &proc_root); // 指定根结点的回调为 proc_root
                    //创建文件系统根目录项dentry实例,并关联proc_inode.vfs_inode成员
                    -> 		if (de->proc_fops) {
                                if (S_ISREG(inode->i_mode)) {
                    #ifdef CONFIG_COMPAT
                                    if (!de->proc_fops->compat_ioctl)
                                        inode->i_fop =
                                            &proc_reg_file_ops_no_compat;
                                    else
                    #endif
                                        inode->i_fop = &proc_reg_file_ops;
                                }
                                else
                                    inode->i_fop = de->proc_fops;
                            }
                ```
                    struct proc_dir_entry proc_root = {
                        .low_ino	= PROC_ROOT_INO, 
                        .namelen	= 5, 
                        .name		= "/proc",
                        .mode		= S_IFDIR | S_IRUGO | S_IXUGO, 
                        .nlink		= 2, 
                        .count		= ATOMIC_INIT(1),
                        .proc_iops	= &proc_root_inode_operations,  // proc_inode.iops
                        .proc_fops	= &proc_root_operations,    //proc_inode.fops
                        .parent		= &proc_root,
                    };
                ```
                总结: 
                    1. 挂载操作将创建proc文件系统超级块super_block实例,文件系统根目录项dentry实例,以及proc_inode实例。proc_inode实例中包含节点inode结构体成员,其i_op、i_fop成员分别指向proc_root实例中proc_iops、proc_fops指向的实例。
                    2. 对于根节点,proc_iops、proc_fops指向的实例是专用的,它们分别为proc_root_inode_operations、proc_root_operations。
                    3. 用户打开proc文件系统中文件时,将为每个路径分量(目录项)创建dentry和proc_inode实例,并建立proc_inode实例与proc_dir_entry实例之间的关联。

        2.3 添加普通目录项
            向proc文件系统添加普通目录项的接口函数为proc_mkdir()
            proc_mkdir
            -> proc_mkdir_mode(name, S_IRUGO | S_IXUGO, parent);
                -> proc_create(&parent, name, S_IFDIR | mode, 2); 
                    -> proc_register(parent, ent)
                        -> if (S_ISDIR(dp->mode)) {
                                if (dp->proc_iops == NULL) {
                                    dp->proc_fops = &proc_dir_operations;
                                    dp->proc_iops = &proc_dir_inode_operations;
                                }
                                dir->nlink++;
                            } else if (S_ISLNK(dp->mode)) {
                                if (dp->proc_iops == NULL)
                                    dp->proc_iops = &proc_link_inode_operations;
                            } else if (S_ISREG(dp->mode)) {
                                if (dp->proc_fops == NULL)
                                    dp->proc_fops = &proc_file_operations;
                                if (dp->proc_iops == NULL)
                                    dp->proc_iops = &proc_file_inode_operations;
                            }
                            dir->subdir = dp;
        2.4 打开文件
            在打开文件的操作中将调用proc_get_inode()函数为proc_dir_entry实例创建proc_inode实例,如果实例代表的是普通文件,则inode实例关联的文件操作结构实例设为proc_reg_file_ops,定义如下
            ```
                static const struct file_operations proc_reg_file_ops = {
                    .llseek		= proc_reg_llseek,
                    .read		= proc_reg_read,
                    .write		= proc_reg_write,
                    .poll		= proc_reg_poll,
                    .unlocked_ioctl	= proc_reg_unlocked_ioctl,
                #ifdef CONFIG_COMPAT
                    .compat_ioctl	= proc_reg_compat_ioctl,
                #endif
                    .mmap		= proc_reg_mmap,
                    .open		= proc_reg_open,
                    .release	= proc_reg_release,
                };
            ```
        2.5 打开文件后,需要读写文件,其也就可以调用该接口的read/write接口,参考上述socketfs的dentry_open.

    3. sysfs
        sys文件系统与proc文件系统一样,通过内核数据结构实例,组织成文件系统,它由kernfs_node结构体实例构成。
        kernfs文件系统并不是一个完整的文件系统,它只是提供构建文件系统的基础组件及相关的接口函数,使用kernfs文件系统的子系统需要定义并注册文件系统类型file_system_type实例。其过程与procfs文件基本类似就不单独介绍。
}

;{    NETFILTER
    1. ebfilter
        1.1 挂载点
            // 文件:include/linux/netfilter_ipv4.h
            #define NF_IP_PRE_ROUTING   0
            #define NF_IP_LOCAL_IN      1
            #define NF_IP_FORWARD       2
            #define NF_IP_LOCAL_OUT     3
            #define NF_IP_POST_ROUTING  4

        1.2 NF_HOOK宏
            static inline int nf_hook_thresh(int pf, unsigned int hook,
				 struct sk_buff *skb,
				 struct net_device *indev,
				 struct net_device *outdev,
				 int (*okfn)(struct sk_buff *), int thresh,
				 int cond)
                {
                    if (!cond)
                        return 1;
                #ifndef CONFIG_NETFILTER_DEBUG
                    if (list_empty(&nf_hooks[pf][hook]))
                        return 1;
                #endif
                    return nf_hook_slow(pf, hook, skb, indev, outdev, okfn, thresh);
                }

            #define NF_HOOK_THRESH(pf, hook, skb, indev, outdev, okfn, thresh)	       \
                ({int __ret;								       \
                if ((__ret=nf_hook_thresh(pf, hook, (skb), indev, outdev, okfn, thresh, 1)) == 1)\
                    __ret = (okfn)(skb);						       \
                __ret;})

            #define NF_HOOK(pf, hook, skb, indev, outdev, okfn) \
                    NF_HOOK_THRESH(pf, hook, skb, indev, outdev, okfn, INT_MIN)

            /*    如果nf_hooks[pf][hook]所指向的链表为空（即该钩子上没有挂处理函数）,则直接调用okfn;否则,则调用net/core/netfilter.c::nf_hook_slow()转入Netfilter的处理。  */

        1.3 br_handle_frame
            br_handle_frame 主要有两个分支有NF_HOOK的调用的,如下: 
            |---link-local----      NF_HOOK(NFPROTO_BRIDGE,NF_BR_LOCAL_IN,..,br_handle_local_finish) 
            |---forward--          NF_HOOK(NFPROTO_BRIDGE, NF_BR_PRE_ROUTING, ...,br_handle_frame_finish)
            link-local :如果目的mac是本地链路地址,则会调用br_handle_local_finish

            1.3.1 br_handle_local_finish
                //根据报文的源MAC来更新FDB表项
                if (NF_HOOK(PF_BRIDGE, NF_BR_LOCAL_IN, skb, skb->dev,
                        NULL, br_handle_local_finish))
                -> br_fdb_update(p->br, p, eth_hdr(skb)->h_source);

            1.3.2 br_handle_frame_finish
                    -> br_fdb_update
                    -> br_pass_frame_up
                    -> br_forward
                    -> br_flood_forward

                    br_handle_frame_finish 这个函数对数据包的dmac进行判断,然后走不同的处理函数 。
                    A.bridge it,如果dmac是在网桥的别的端口,复制一份帧到dmac所在的端口---->br_forward 
                    B.flood it over all the forwarding bridge ports,如果dmac地址是网桥不知道的,就泛 ---->br_flood_forward
                    C.pass it to the higher protocol code,如果dmac是网桥的,或者网桥其中一个端口的---->br_pass_frame_up
                    D.ignore it,dmac在进来的端口的这一边的,即dmac能在进来端口的mac地址表中找到---->br_forward
                    总之数据包发送有两个地方,一个是转发出去br_forward或者br_flood_forward,一个是发往本地br_pass_frame_up。

                ```
                switch (p->state) 
                    case BR_STATE_FORWARDING:
                        rhook = rcu_dereference(br_should_route_hook);
                        if (rhook != NULL) {
                            if (rhook(skb))
                                return skb;
                            dest = eth_hdr(skb)->h_dest;
                        }
                        /* fall through */
                    case BR_STATE_LEARNING:
                        if (!compare_ether_addr(p->br->dev->dev_addr, dest))
                            skb->pkt_type = PACKET_HOST;

                        NF_HOOK(PF_BRIDGE, NF_BR_PRE_ROUTING, skb, skb->dev, NULL,
                            br_handle_frame_finish);
                        break;
                ```

                1.3.2.1 br_forward
                    -> __br_forward(to, skb);
                        -> 	NF_HOOK(PF_BRIDGE, NF_BR_FORWARD, skb, indev, skb->dev,
                                br_forward_finish);
                                -> NF_HOOK(PF_BRIDGE, NF_BR_POST_ROUTING, skb, NULL, skb->dev,
                                    br_dev_queue_push_xmit);
                                        -> dev_queue_xmit(skb); // 把报文发出去
                    ```
                        if (should_deliver(to, skb)) 
                            __br_forward(to, skb);
                    ```

                1.3.2.2 br_pass_frame_up
                    -> NF_HOOK(PF_BRIDGE, NF_BR_LOCAL_IN, skb, indev, NULL,
		                netif_receive_skb); // 接着还是回到netif_receive_skb函数起始的地方.
                        ```
                            indev = skb->dev;
	                        skb->dev = br->dev; // 不过这里的dev被修改成br-lan下挂的设备eth,重新走一趟
                        ```

    2. ipfilter
        0. inet_add_protocol初始化inet_protos
            inet_init(void)
                -> inet_add_protocol
                    -> inet_protos[hash] = prot;
                ```
                    if (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)
                        printk(KERN_CRIT "inet_init: Cannot add ICMP protocol\n");
                    if (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)
                        printk(KERN_CRIT "inet_init: Cannot add UDP protocol\n");
                    if (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)
                        printk(KERN_CRIT "inet_init: Cannot add TCP protocol\n");
                        static struct net_protocol tcp_protocol = {
                            .handler =	tcp_v4_rcv,
                            .err_handler =	tcp_v4_err,
                            .gso_send_check = tcp_v4_gso_send_check,
                            .gso_segment =	tcp_tso_segment,
                            .no_policy =	1,
                        };
                #ifdef CONFIG_IP_MULTICAST
                    if (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)
                        printk(KERN_CRIT "inet_init: Cannot add IGMP protocol\n");
                #endif
                ```

        1. ip_rcv
            大体流程:
                1. 丢弃PACKET_OTHERHOST类型的包
                    1. dmac不是本主机且没有被bridge消耗
                    2. 携带vlan且没有对应的vlan子接口,没有被bridge消耗
                    3. 其他:
                2. 以最小长度获取ip头,校验版本和头部长度
                3. 以ip头携带的头部长度获取ip头,校验skb长度、ip头长度以及ip包总长度
                4. 通过iptables的PRE_ROUTING链,继续则进入ip_rcv_finish
            -> NF_HOOK(PF_INET, NF_IP_PRE_ROUTING, skb, dev, NULL,
		       ip_rcv_finish);
                -> ip_route_input(skb, iph->daddr, iph->saddr, iph->tos, skb->dev);
                    -> ip_route_input_slow
                        初始化入口流量的函数指针:入口IP流量是由ip_rcv_finish处理,该函数参照路由表来决定将封包送往本地还是丢弃。在路由缓存不命中的情况下,ip_route_input_slow可以创建以下三种dst->input和dst->output的组合:
                            1. 当封包被转发时,函数将dst->input初始化为ip_forward,将dst->output初始化为ip_output。所以dst_output将调用ip_forward,而在ip_forward的结尾处间接调用dst_output,即ip_output
                            2. 如果封包被送往本地,函数将dst->input初始化为ip_local_deliver,此时不需要初始化dst->output,但它还是被初始化为ip_rt_error,用于检测在处理送往本地封包的dst->ouput是否被错误调用
                            3. 如果根据路由表得出目的地址不可达,dst->input被初始化为ip_error,这将产生一个ICMP消息,消息类型依赖于路由查找返回的准确结果,因为ip_error将skb缓冲区释放,所以不需要初始化dst->output,因为即使在犯错情况下它也不会被调用

                        -> local_input: rth->u.dst.input= ip_local_deliver;
                                        rth->u.dst.output= ip_rt_bug;
                        -> forward: 
                            __mkroute_input
                                -> rth->u.dst.input = ip_forward;
                                   rth->u.dst.output = ip_output;
                -> dst_input
                    -> skb->dst->input(skb);    //该函数确定了下一步对数据包的处理,根据数据包的目的地地址,skb->dst->input字段的信息主要由路由处理流程确定,可能是往本地协议栈上传就调用 ip_local_deliver,如果是转发就调用ip_forward */
                        -> local_input:
                            ip_local_deliver
                                -> NF_HOOK(PF_INET, NF_IP_LOCAL_IN, skb, skb->dev, NULL,
                                    ip_local_deliver_finish);
                                    -> if ((ipprot = rcu_dereference(inet_protos[hash])) != NULL) 
                                            ret = ipprot->handler(skb); // 比如tcp的tcp_v4_rcv
                                            -> tcp_v4_rcv
                                                -> tcp_v4_do_rcv
                                                    -> tcp_rcv_state_process
                                                        -> tcp_data_queue
                                                            -> __skb_queue_tail(&sk->sk_receive_queue, skb);
                        ```
                            tcp_v4_connect
                                -> ip_route_connect
                                    -> __ip_route_output_key
                                        -> ip_route_output_slow
                                            -> ip_mkroute_output
                                                -> rt_intern_hash
                                                    -> arp_bind_neighbour
                                                        -> __neigh_lookup_errno
                                                            -> neigh_create
                                                                -> if (tbl->constructor && (error = tbl->constructor(n)) 0) 

                            arp_init
                                -> neigh_table_init(&arp_tbl);
                                struct neigh_table arp_tbl = {
                                    .family =	AF_INET,
                                    .entry_size =	sizeof(struct neighbour) + 4,
                                    .key_len =	4,
                                    .hash =		arp_hash,
                                    .constructor =	arp_constructor,
                                    ...
                                }

                                static struct neigh_ops arp_hh_ops = {
                                    .family =		AF_INET,
                                    .solicit =		arp_solicit,
                                    .error_report =		arp_error_report,
                                    .output =		neigh_resolve_output,
                                    .connected_output =	neigh_connected_output,
                                    .hh_output =		dev_queue_xmit,
                                    .queue_xmit =		dev_queue_xmit,
                                };

                                static int arp_constructor(struct neighbour *neigh)
                                {
                                    if (dev->header_ops->cache)
                                        neigh->ops = &arp_hh_ops;
                                    else
                                        neigh->ops = &arp_generic_ops;
                                }
                        ```

                        -> forward:
                            ip_forward
                                -> NF_HOOK(PF_INET, NF_IP_FORWARD, skb, skb->dev, rt->u.dst.dev,
		                                ip_forward_finish)
                                    -> dst_output
                                        -> skb->dst->output(skb);
                                            ip_output
                                                -> NF_HOOK_COND(PF_INET, NF_IP_POST_ROUTING, skb, NULL, dev,
			                                        ip_finish_output, !(IPCB(skb)->flags & IPSKB_REROUTED))
                                                    -> ip_finish_output2
                                                        -> dst->neighbour->output(skb) // 参考上面arp_hh_ops
                                                            neigh_resolve_output
                                                                -> neigh->ops->queue_xmit
                                                                    dev_queue_xmit // 参考上面arp_hh_ops QOS:TC
                                                                        -> dev_hard_start_xmit
}
 
;{  tcp/IP
    1. inet_init
        1. 初始化net_families[ops->family] = ops;
            1. sock_register
                -> sock_register(&pppox_proto_family);
                -> (void)sock_register(&inet_family_ops);
                    ```
                        static struct net_proto_family inet_family_ops = {
                            .family = PF_INET,
                            .create = inet_create,
                            .owner	= THIS_MODULE,
                        };
                    ```
                -> sock_register(&inet6_family_ops);

        2. 初始化inet_register_protosw
            inet_init
            -> for (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)
		            INIT_LIST_HEAD(r);
            -> for (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)
		            inet_register_protosw(q);
                        ```
                            last_perm = &inetsw[p->type];
                            list_add_rcu(&p->list, last_perm); // 将p->list挂在last_perm下面,即将inetsw_array挂在inetsw下面
                        ```
            ```
                static struct inet_protosw inetsw_array[] =
                {
                    {
                        .type =       SOCK_STREAM,
                        .protocol =   IPPROTO_TCP,
                        .prot =       &tcp_prot,
                        .ops =        &inet_stream_ops,
                        .capability = -1,
                        .no_check =   0,
                        .flags =      INET_PROTOSW_PERMANENT |
                                INET_PROTOSW_ICSK,
                    },
                    {
                        .type =       SOCK_DGRAM,
                        .protocol =   IPPROTO_UDP,
                        .prot =       &udp_prot,
                        .ops =        &inet_dgram_ops,
                        .capability = -1,
                        .no_check =   UDP_CSUM_DEFAULT,
                        .flags =      INET_PROTOSW_PERMANENT,
                    },
                    {
                        .type =       SOCK_RAW,
                        .protocol =   IPPROTO_IP,	/* wild card */
                        .prot =       &raw_prot,
                        .ops =        &inet_sockraw_ops,
                        .capability = CAP_NET_RAW,
                        .no_check =   UDP_CSUM_DEFAULT,
                        .flags =      INET_PROTOSW_REUSE,
                    }
                };
            ```

        2. sys_socket(int family, int type, int protocol) // 使用前面初始化的初始化net_families
            -> sock_create(family, type, protocol, &sock);
                -> pf = rcu_dereference(net_families[family]);
                -> err = pf->create(net, sock, protocol); // net_families[ops->family].create, 比如ipv4 inet_create
                    -> inet_create
                        ```
                        	list_for_each_rcu(p, &inetsw[sock->type]) 
                            answer = list_entry(p, struct inet_protosw, list);
                                if (protocol == answer->protocol) 
                        ```
                        -> sock->ops = answer->ops; // inetsw_array中的inet_stream_ops

        3. sys_sendmsg
            -> sock->ops->sendmsg
                -> inet_stream_ops->sendmsg
                    tcp_sendmsg

        4. sys_read
            -> vfs_read
                -> do_sync_read
                    -> ret = filp->f_op->aio_read(&kiocb, &iov, 1, kiocb.ki_pos); //socket_file_ops
                        -> socket_file_ops->aio_read
                            -> do_sock_read
                                -> __sock_recvmsg
                                    -> sock->ops = answer->ops; // inetsw_array中的inet_stream_ops
                                        -> sock_common_recvmsg
                                            -> sk->sk_prot->recvmsg // inetsw_array中的inet_stream_ops的tcp_prot
                                                -> tcp_recvmsg
                                                    ```
                                                        skb = skb_peek(&sk->sk_receive_queue); //前面tcp_rcv收到的数据
                                                        err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, used); // 拷贝数据到应用层
                                                    ```

}